{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5e61d4-9181-4950-ba4e-0293caabed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "from io import StringIO\n",
    "from functools import wraps\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pstats\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pympler import asizeof\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from time_res_util import get_compiled_NF_model\n",
    "from momentum_prediction_util import load_defaultdict, SiPMSignalProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453c96b8-3afc-48b2-9da5-bdd69f5c151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_function(func):\n",
    "    \"\"\"\n",
    "    Decorator to profile a specific function using cProfile\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        profiler = cProfile.Profile()\n",
    "        try:\n",
    "            return profiler.runcall(func, *args, **kwargs)\n",
    "        finally:\n",
    "            s = StringIO()\n",
    "            stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n",
    "            stats.print_stats(20)  # Print top 20 time-consuming operations\n",
    "            print(s.getvalue())\n",
    "    return wrapper\n",
    "\n",
    "'''MEMORY PROFILING'''\n",
    "import linecache\n",
    "import os\n",
    "import tracemalloc\n",
    "\n",
    "def display_top(snapshot, key_type='lineno', limit=3):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\n",
    "\n",
    "tracemalloc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4671432-d096-45c6-b89a-1091cc3f240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/vossenlab/rck32/ML_venv/lib64/python3.9/site-packages/normflows/core.py:213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "inputProcessedData = \"./data/processed_data/jan_13_new_analyze_10events.json\"\n",
    "model_compile = get_compiled_NF_model()\n",
    "processed_data = load_defaultdict(inputProcessedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbec99c5-43dc-4b7b-9a90-dbf5169987e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data in new_prepare_nn_input...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:16<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling took 16.723486185073853 seconds\n",
      "Processing signal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "normalizing_flow = model_compile\n",
    "batch_size=50000\n",
    "device='cuda'\n",
    "pixel_threshold = 5\n",
    "\n",
    "all_context = []\n",
    "all_time_pixels = []\n",
    "all_metadata = []\n",
    "num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
    "print(\"Processing data in new_prepare_nn_input...\")\n",
    "for event_idx, event_data in tqdm(processed_data.items()):\n",
    "    for stave_idx, stave_data in event_data.items():\n",
    "        for layer_idx, layer_data in stave_data.items():\n",
    "            for segment_idx, segment_data in layer_data.items():\n",
    "                trueID_list = []\n",
    "                for particle_id, particle_data in segment_data.items():\n",
    "#                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
    "#                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
    "                    base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
    "                                                dtype=torch.float32)\n",
    "                    base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
    "                                                    dtype=torch.float32)\n",
    "                    base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
    "                                                    dtype=torch.float32)\n",
    "                    if particle_data['trueID'] not in  trueID_list:\n",
    "                        trueID_list.append(particle_data['trueID'])\n",
    "                    for SiPM_idx in range(2):\n",
    "                        z_pos = particle_data['z_pos']\n",
    "                        context = base_context.clone()\n",
    "                        context[0] = z_pos\n",
    "                        num_pixel_tag = num_pixel_list[SiPM_idx]\n",
    "                        all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        if(SiPM_idx == 0):\n",
    "                            all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        else:\n",
    "                            all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
    "                        fields = [\n",
    "                            'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
    "                            'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
    "                            'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
    "                            'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
    "                            'KMU_endpoint_y', 'KMU_endpoint_z'\n",
    "                        ]\n",
    "\n",
    "                        # Print types of each particle_data field\n",
    "#                             for field in fields:\n",
    "#                                 value = particle_data.get(field, None)\n",
    "#                                 print(f\"{field}: {type(value)}\")\n",
    "\n",
    "#                             # Print the type of len(trueID_list)\n",
    "#                             print(f\"len(trueID_list): {type(len(trueID_list))}\")\n",
    "\n",
    "                        all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
    "\n",
    "all_context = torch.cat(all_context)\n",
    "all_time_pixels = torch.cat(all_time_pixels)\n",
    "\n",
    "print(\"Sampling data...\")\n",
    "sampled_data = []\n",
    "begin = time.time()\n",
    "for i in tqdm(range(0, len(all_context), batch_size)):\n",
    "    batch_end = min(i + batch_size, len(all_context))\n",
    "    batch_context = all_context[i:batch_end].to(device)\n",
    "    batch_time_pixels = all_time_pixels[i:batch_end]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
    "\n",
    "    sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
    "end = time.time()\n",
    "print(f\"sampling took {end - begin} seconds\")\n",
    "print(\"Processing signal...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40b862ca-5066-4057-82d9-2f334924dc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DF took 36.81203651428223 seconds\n"
     ]
    }
   ],
   "source": [
    "# VARIABLES FOR SAVING DATA AS DF\n",
    "processer = SiPMSignalProcessor()\n",
    "rows = []\n",
    "\n",
    "seen_keys = []\n",
    "curr_key = (-1,-1,-1,-1)\n",
    "\n",
    "current_samples = [[],[]] \n",
    "processor = SiPMSignalProcessor()\n",
    "\n",
    "translated_trueID = 0\n",
    "trueID_dict_running_idx = 0\n",
    "trueID_dict = {}\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "sample_idx = 0\n",
    "for (event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, momentum,trueID,truePID,hitID,hitPID,theta,phi,strip_x,strip_y,strip_z,trueID_list_len,hit_x,hit_y,hit_z,KMU_trueID,KMU_truePID,KMU_true_phi,KMU_true_momentum_mag,KMU_endpoint_x,KMU_endpoint_y,KMU_endpoint_z), sample in zip(all_metadata, sampled_data):\n",
    "\n",
    "    #progress bar\n",
    "    floor_percent = int(np.floor(len(sampled_data) / 100))\n",
    "    if(sample_idx % floor_percent == 0):\n",
    "        curr_time = time.time()\n",
    "        print(f\"Signal Processing is now {int(np.floor(sample_idx / len(sampled_data) * 100))}% complete (time elapsed: {curr_time - begin})\")\n",
    "        clear_output(wait = True)\n",
    "    sample_idx += 1\n",
    "\n",
    "    # Work with all samples of one SiPM together\n",
    "    key = (event_idx, stave_idx, layer_idx, segment_idx)\n",
    "    if key in seen_keys:\n",
    "        if key == curr_key:\n",
    "            current_samples[SiPM_idx].append(sample)\n",
    "        else:\n",
    "            continue\n",
    "            print(f\"ERROR: key: {key} | curr_key: {curr_key}\")\n",
    "    # First key\n",
    "    elif curr_key == (-1,-1,-1,-1):\n",
    "        current_samples[SiPM_idx].append(sample)\n",
    "        seen_keys.append(key)\n",
    "        curr_key = key\n",
    "    # End of curr_key: perform calc\n",
    "    else:\n",
    "        #calculate photon stuff on current_samples\n",
    "\n",
    "        '''IMPLEMENTING PREDICTION INPUT PULSE SEGMENT BY SEGMENT'''\n",
    "        curr_event_idx = curr_key[0]\n",
    "        curr_stave_idx = curr_key[1]\n",
    "        curr_layer_idx = curr_key[2]\n",
    "        curr_segment_idx = curr_key[3]\n",
    "        for curr_SiPM_idx in range(2):\n",
    "            trigger = False\n",
    "            photon_times = np.array(current_samples[curr_SiPM_idx]) * 10 **(-9)\n",
    "            if(len(photon_times) > 0):\n",
    "                time_arr,waveform = processor.generate_waveform(photon_times)\n",
    "                timing = processer.get_pulse_timing(waveform,threshold = pixel_threshold)\n",
    "                if(timing is not None):\n",
    "                    #scale inputs to avoid exploding gradients\n",
    "                    curr_charge = processor.integrate_charge(waveform) * 1e6\n",
    "                    curr_timing = timing * 1e8\n",
    "                    trigger = True\n",
    "                #skip segments that don't pass the threshold\n",
    "                else:\n",
    "                    continue\n",
    "            #skip segments with no photon hits\n",
    "            else:\n",
    "                continue\n",
    "            if(trueID_list_len > 1):\n",
    "                translated_trueID = -1\n",
    "            else:\n",
    "                if((event_idx,trueID) not in trueID_dict):\n",
    "                    trueID_dict[(event_idx,trueID)] = trueID_dict_running_idx\n",
    "                    trueID_dict_running_idx += 1\n",
    "                translated_trueID = trueID_dict[(event_idx,trueID)]\n",
    "            new_row = {\n",
    "                \"event_idx\"      : curr_event_idx,\n",
    "                \"stave_idx\"      : curr_stave_idx,\n",
    "                \"layer_idx\"      : curr_layer_idx,\n",
    "                \"segment_idx\"    : curr_segment_idx,\n",
    "                \"SiPM_idx\"    : curr_SiPM_idx,\n",
    "                \"trueID\"         : translated_trueID,\n",
    "                \"truePID\"        : trueID,\n",
    "                \"hitID\"          : hitID,\n",
    "                \"P\"              : momentum,\n",
    "                \"Theta\"          : theta,\n",
    "                \"Phi\"            : phi,\n",
    "                \"strip_x\"        : strip_z,\n",
    "                \"strip_y\"        : strip_x,\n",
    "                \"strip_z\"        : strip_y,\n",
    "                \"hit_x\"          : hit_x,\n",
    "                \"hit_y\"          : hit_y,\n",
    "                \"hit_z\"          : hit_z,\n",
    "                \"KMU_endpoint_x\" : KMU_endpoint_x,\n",
    "                \"KMU_endpoint_y\" : KMU_endpoint_y,\n",
    "                \"KMU_endpoint_z\" : KMU_endpoint_z,\n",
    "                \"Charge\"         : curr_charge,\n",
    "                \"Time\"           : curr_timing\n",
    "            }\n",
    "            rows.append(new_row)\n",
    "        ''' END IMPLEMENTATION '''\n",
    "        #reset current samples for new key\n",
    "        seen_keys.append(key)\n",
    "        current_samples = [[],[]]\n",
    "        current_samples.append(sample)\n",
    "        curr_key = key\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "ret_df = pd.DataFrame(rows)\n",
    "print(f\"Creating DF took {end - begin} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a27c5-a11b-4ef4-98b2-c03c30a2edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(my_function)\n",
    "profiler.run('my_function()')\n",
    "profiler.print_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_venv",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
