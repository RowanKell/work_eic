{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5e61d4-9181-4950-ba4e-0293caabed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "from io import StringIO\n",
    "from functools import wraps\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pstats\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pympler import asizeof\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "from time_res_util import get_compiled_NF_model\n",
    "from momentum_prediction_util import load_defaultdict, SiPMSignalProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453c96b8-3afc-48b2-9da5-bdd69f5c151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_function(func):\n",
    "    \"\"\"\n",
    "    Decorator to profile a specific function using cProfile\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        profiler = cProfile.Profile()\n",
    "        try:\n",
    "            return profiler.runcall(func, *args, **kwargs)\n",
    "        finally:\n",
    "            s = StringIO()\n",
    "            stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n",
    "            stats.print_stats(20)  # Print top 20 time-consuming operations\n",
    "            print(s.getvalue())\n",
    "    return wrapper\n",
    "\n",
    "'''MEMORY PROFILING'''\n",
    "import linecache\n",
    "import os\n",
    "import tracemalloc\n",
    "\n",
    "def display_top(snapshot, key_type='lineno', limit=3):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\n",
    "\n",
    "tracemalloc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4671432-d096-45c6-b89a-1091cc3f240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/vossenlab/rck32/ML_venv/lib64/python3.9/site-packages/normflows/core.py:213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "inputProcessedData = \"./data/processed_data/jan_13_new_analyze_10events.json\"\n",
    "model_compile = get_compiled_NF_model()\n",
    "processed_data = load_defaultdict(inputProcessedData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aebf93-3744-4f63-a81d-9ea33a90e2c8",
   "metadata": {},
   "source": [
    "## Current fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb9ec1e6-6022-4f5f-9973-f4e6f81e8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newer_prepare_nn_input(processed_data = processed_data, normalizing_flow=model_compile, batch_size=50000, device='cuda',pixel_threshold = 5):\n",
    "    processer = SiPMSignalProcessor()\n",
    "    \n",
    "    all_context = []\n",
    "    all_time_pixels = []\n",
    "    all_metadata = []\n",
    "    num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
    "    print(\"Processing data in new_prepare_nn_input...\")\n",
    "    for event_idx, event_data in tqdm(processed_data.items()):\n",
    "        for stave_idx, stave_data in event_data.items():\n",
    "            for layer_idx, layer_data in stave_data.items():\n",
    "                for segment_idx, segment_data in layer_data.items():\n",
    "                    trueID_list = []\n",
    "                    for particle_id, particle_data in segment_data.items():\n",
    "#                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
    "#                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
    "                        base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
    "                                                    dtype=torch.float32)\n",
    "                        base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
    "                                                        dtype=torch.float32)\n",
    "                        base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
    "                                                        dtype=torch.float32)\n",
    "                        if particle_data['trueID'] not in  trueID_list:\n",
    "                            trueID_list.append(particle_data['trueID'])\n",
    "                        for SiPM_idx in range(2):\n",
    "                            z_pos = particle_data['z_pos']\n",
    "                            context = base_context.clone()\n",
    "                            context[0] = z_pos\n",
    "                            num_pixel_tag = num_pixel_list[SiPM_idx]\n",
    "                            all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            if(SiPM_idx == 0):\n",
    "                                all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            else:\n",
    "                                all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
    "                            fields = [\n",
    "                                'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
    "                                'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
    "                                'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
    "                                'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
    "                                'KMU_endpoint_y', 'KMU_endpoint_z'\n",
    "                            ]\n",
    "\n",
    "                            # Print types of each particle_data field\n",
    "#                             for field in fields:\n",
    "#                                 value = particle_data.get(field, None)\n",
    "#                                 print(f\"{field}: {type(value)}\")\n",
    "\n",
    "#                             # Print the type of len(trueID_list)\n",
    "#                             print(f\"len(trueID_list): {type(len(trueID_list))}\")\n",
    "\n",
    "                            all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
    "\n",
    "    all_context = torch.cat(all_context)\n",
    "    all_time_pixels = torch.cat(all_time_pixels)\n",
    "    \n",
    "    print(\"Sampling data...\")\n",
    "    sampled_data = []\n",
    "    begin = time.time()\n",
    "    for i in tqdm(range(0, len(all_context), batch_size)):\n",
    "        batch_end = min(i + batch_size, len(all_context))\n",
    "        batch_context = all_context[i:batch_end].to(device)\n",
    "        batch_time_pixels = all_time_pixels[i:batch_end]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
    "        \n",
    "        sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
    "    end = time.time()\n",
    "    print(f\"sampling took {end - begin} seconds\")\n",
    "    print(\"Processing signal...\")\n",
    "    \n",
    "    \n",
    "    # VARIABLES FOR SAVING DATA AS DF\n",
    "    processer = SiPMSignalProcessor()\n",
    "    rows = []\n",
    "\n",
    "    seen_keys = set()\n",
    "    curr_key = (-1,-1,-1,-1)\n",
    "\n",
    "    current_samples = [[],[]] \n",
    "    processor = SiPMSignalProcessor()\n",
    "\n",
    "    translated_trueID = 0\n",
    "    trueID_dict_running_idx = 0\n",
    "    trueID_dict = {}\n",
    "\n",
    "    begin = time.time()\n",
    "\n",
    "#     sample_idx = 0\n",
    "    for (event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, momentum,trueID,truePID,hitID,hitPID,theta,phi,strip_x,strip_y,strip_z,trueID_list_len,hit_x,hit_y,hit_z,KMU_trueID,KMU_truePID,KMU_true_phi,KMU_true_momentum_mag,KMU_endpoint_x,KMU_endpoint_y,KMU_endpoint_z), sample in zip(all_metadata, sampled_data):\n",
    "\n",
    "        # Work with all samples of one SiPM together\n",
    "        key = (event_idx, stave_idx, layer_idx, segment_idx)\n",
    "        \n",
    "        if key in seen_keys:\n",
    "            if key == curr_key:\n",
    "                current_samples[SiPM_idx].append(sample)\n",
    "            else:\n",
    "                continue\n",
    "                print(f\"ERROR: key: {key} | curr_key: {curr_key}\")\n",
    "        # First key\n",
    "        elif curr_key == (-1,-1,-1,-1):\n",
    "            current_samples[SiPM_idx].append(sample)\n",
    "            seen_keys.add(key)\n",
    "            curr_key = key\n",
    "        # End of curr_key: perform calc\n",
    "        else:\n",
    "            #calculate photon stuff on current_samples\n",
    "\n",
    "            '''IMPLEMENTING PREDICTION INPUT PULSE SEGMENT BY SEGMENT'''\n",
    "            curr_event_idx = curr_key[0]\n",
    "            curr_stave_idx = curr_key[1]\n",
    "            curr_layer_idx = curr_key[2]\n",
    "            curr_segment_idx = curr_key[3]\n",
    "            for curr_SiPM_idx in range(2):\n",
    "                trigger = False\n",
    "                photon_times = np.array(current_samples[curr_SiPM_idx]) * 10 **(-9)\n",
    "                if(len(photon_times) > 0):\n",
    "                    time_arr,waveform = processor.generate_waveform(photon_times)\n",
    "                    timing = processer.get_pulse_timing(waveform,threshold = pixel_threshold)\n",
    "                    if(timing is not None):\n",
    "                        #scale inputs to avoid exploding gradients\n",
    "                        curr_charge = processor.integrate_charge(waveform) * 1e6\n",
    "                        curr_timing = timing * 1e8\n",
    "                        trigger = True\n",
    "                    #skip segments that don't pass the threshold\n",
    "                    else:\n",
    "                        continue\n",
    "                #skip segments with no photon hits\n",
    "                else:\n",
    "                    continue\n",
    "                if(trueID_list_len > 1):\n",
    "                    translated_trueID = -1\n",
    "                else:\n",
    "                    if((event_idx,trueID) not in trueID_dict):\n",
    "                        trueID_dict[(event_idx,trueID)] = trueID_dict_running_idx\n",
    "                        trueID_dict_running_idx += 1\n",
    "                    translated_trueID = trueID_dict[(event_idx,trueID)]\n",
    "                new_row = {\n",
    "                    \"event_idx\"      : curr_event_idx,\n",
    "                    \"stave_idx\"      : curr_stave_idx,\n",
    "                    \"layer_idx\"      : curr_layer_idx,\n",
    "                    \"segment_idx\"    : curr_segment_idx,\n",
    "                    \"SiPM_idx\"    : curr_SiPM_idx,\n",
    "                    \"trueID\"         : translated_trueID,\n",
    "                    \"truePID\"        : trueID,\n",
    "                    \"hitID\"          : hitID,\n",
    "                    \"P\"              : momentum,\n",
    "                    \"Theta\"          : theta,\n",
    "                    \"Phi\"            : phi,\n",
    "                    \"strip_x\"        : strip_z,\n",
    "                    \"strip_y\"        : strip_x,\n",
    "                    \"strip_z\"        : strip_y,\n",
    "                    \"hit_x\"          : hit_x,\n",
    "                    \"hit_y\"          : hit_y,\n",
    "                    \"hit_z\"          : hit_z,\n",
    "                    \"KMU_endpoint_x\" : KMU_endpoint_x,\n",
    "                    \"KMU_endpoint_y\" : KMU_endpoint_y,\n",
    "                    \"KMU_endpoint_z\" : KMU_endpoint_z,\n",
    "                    \"Charge\"         : curr_charge,\n",
    "                    \"Time\"           : curr_timing\n",
    "                }\n",
    "                rows.append(new_row)\n",
    "            ''' END IMPLEMENTATION '''\n",
    "            #reset current samples for new key\n",
    "            seen_keys.add(key)\n",
    "            current_samples = [[],[]]\n",
    "            current_samples.append(sample)\n",
    "            curr_key = key\n",
    "                \n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    ret_df = pd.DataFrame(rows)\n",
    "    print(f\"Creating DF took {end - begin} seconds\")\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53f230-84f3-4c05-8f46-53da55ef2d86",
   "metadata": {},
   "source": [
    "## Test: claude way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bc15b6d-467d-4aeb-b9b8-c6297751f26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data in new_prepare_nn_input...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling took 13.836615085601807 seconds\n",
      "Processing signal...\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "# Create a key function that extracts the grouping fields\n",
    "def get_key(item):\n",
    "    metadata, _ = item\n",
    "    return metadata[:4]  # event_idx, stave_idx, layer_idx, segment_idx\n",
    "\n",
    "def test_newer_prepare_nn_input(processed_data = processed_data, normalizing_flow=model_compile, batch_size=50000, device='cuda',pixel_threshold = 5):\n",
    "    processer = SiPMSignalProcessor()\n",
    "    \n",
    "    all_context = []\n",
    "    all_time_pixels = []\n",
    "    all_metadata = []\n",
    "    num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
    "    print(\"Processing data in new_prepare_nn_input...\")\n",
    "    for event_idx, event_data in tqdm(processed_data.items()):\n",
    "        for stave_idx, stave_data in event_data.items():\n",
    "            for layer_idx, layer_data in stave_data.items():\n",
    "                for segment_idx, segment_data in layer_data.items():\n",
    "                    trueID_list = []\n",
    "                    for particle_id, particle_data in segment_data.items():\n",
    "#                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
    "#                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
    "                        base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
    "                                                    dtype=torch.float32)\n",
    "                        base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
    "                                                        dtype=torch.float32)\n",
    "                        base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
    "                                                        dtype=torch.float32)\n",
    "                        if particle_data['trueID'] not in  trueID_list:\n",
    "                            trueID_list.append(particle_data['trueID'])\n",
    "                        for SiPM_idx in range(2):\n",
    "                            z_pos = particle_data['z_pos']\n",
    "                            context = base_context.clone()\n",
    "                            context[0] = z_pos\n",
    "                            num_pixel_tag = num_pixel_list[SiPM_idx]\n",
    "                            all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            if(SiPM_idx == 0):\n",
    "                                all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            else:\n",
    "                                all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
    "                            fields = [\n",
    "                                'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
    "                                'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
    "                                'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
    "                                'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
    "                                'KMU_endpoint_y', 'KMU_endpoint_z'\n",
    "                            ]\n",
    "\n",
    "                            # Print types of each particle_data field\n",
    "#                             for field in fields:\n",
    "#                                 value = particle_data.get(field, None)\n",
    "#                                 print(f\"{field}: {type(value)}\")\n",
    "\n",
    "#                             # Print the type of len(trueID_list)\n",
    "#                             print(f\"len(trueID_list): {type(len(trueID_list))}\")\n",
    "\n",
    "                            all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
    "\n",
    "    all_context = torch.cat(all_context)\n",
    "    all_time_pixels = torch.cat(all_time_pixels)\n",
    "    \n",
    "    print(\"Sampling data...\")\n",
    "    sampled_data = []\n",
    "    begin = time.time()\n",
    "    for i in tqdm(range(0, len(all_context), batch_size)):\n",
    "        batch_end = min(i + batch_size, len(all_context))\n",
    "        batch_context = all_context[i:batch_end].to(device)\n",
    "        batch_time_pixels = all_time_pixels[i:batch_end]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
    "        \n",
    "        sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
    "    end = time.time()\n",
    "    print(f\"sampling took {end - begin} seconds\")\n",
    "    print(\"Processing signal...\")\n",
    "    processor = SiPMSignalProcessor()\n",
    "    rows = []\n",
    "    trueID_dict = {}\n",
    "    trueID_dict_running_idx = 0\n",
    "    event_first_hits = {}\n",
    "\n",
    "    # Sort the data first (required for groupby)\n",
    "    sorted_data = sorted(zip(all_metadata, sampled_data), key=get_key)\n",
    "\n",
    "    # Process each group\n",
    "    for key, group in groupby(sorted_data, key=get_key):\n",
    "        event_idx, stave_idx, layer_idx, segment_idx = key\n",
    "\n",
    "        # Initialize arrays for both SiPMs\n",
    "        sipm_samples = [[], []]\n",
    "\n",
    "        # Get the first metadata tuple for this group (they should all be the same within a group)\n",
    "        first_item = next(group)\n",
    "        metadata = first_item[0]\n",
    "        _, _, _, _, _, momentum,trueID,truePID,hitID,hitPID,theta,phi,strip_x,strip_y,strip_z,trueID_list_len,hit_x,hit_y,hit_z,KMU_trueID,KMU_truePID,KMU_true_phi,KMU_true_momentum_mag,KMU_endpoint_x,KMU_endpoint_y,KMU_endpoint_z = metadata\n",
    "        sipm_samples[first_item[0][4]].append(first_item[1])\n",
    "\n",
    "        # Process rest of group\n",
    "        for metadata, sample in group:\n",
    "            sipm_idx = metadata[4]\n",
    "            sipm_samples[sipm_idx].append(sample)\n",
    "\n",
    "        # Process each SiPM's samples\n",
    "        for curr_SiPM_idx in range(2):\n",
    "            if not sipm_samples[curr_SiPM_idx]:\n",
    "                continue\n",
    "\n",
    "            photon_times = np.array(sipm_samples[curr_SiPM_idx]) * 10**(-9)\n",
    "            time_arr, waveform = processor.generate_waveform(photon_times)\n",
    "            timing = processor.get_pulse_timing(waveform, threshold=pixel_threshold)\n",
    "\n",
    "            if timing is None:\n",
    "                continue\n",
    "\n",
    "            curr_charge = processor.integrate_charge(waveform) * 1e6\n",
    "            curr_timing = timing * 1e8\n",
    "            \n",
    "            if event_idx not in event_first_hits or curr_timing < event_first_hits[event_idx][0]:\n",
    "                event_first_hits[event_idx] = (curr_timing, strip_z, strip_x)\n",
    "\n",
    "            # Handle trueID translation\n",
    "            if trueID_list_len > 1:\n",
    "                translated_trueID = -1\n",
    "            else:\n",
    "                event_true_key = (event_idx, trueID)\n",
    "                if event_true_key not in trueID_dict:\n",
    "                    trueID_dict[event_true_key] = trueID_dict_running_idx\n",
    "                    trueID_dict_running_idx += 1\n",
    "                translated_trueID = trueID_dict[event_true_key]\n",
    "\n",
    "            # Create row\n",
    "            rows.append({\n",
    "                \"event_idx\": event_idx,\n",
    "                \"stave_idx\": stave_idx,\n",
    "                \"layer_idx\": layer_idx,\n",
    "                \"segment_idx\": segment_idx,\n",
    "                \"SiPM_idx\": curr_SiPM_idx,\n",
    "                \"trueID\": translated_trueID,\n",
    "                \"truePID\": truePID,\n",
    "                \"hitID\": hitID,\n",
    "                \"P\"              : momentum,\n",
    "                \"Theta\"          : theta,\n",
    "                \"Phi\"            : phi,\n",
    "                \"strip_x\"        : strip_z,\n",
    "                \"strip_y\"        : strip_x,\n",
    "                \"strip_z\"        : strip_y,\n",
    "                \"hit_x\"          : hit_x,\n",
    "                \"hit_y\"          : hit_y,\n",
    "                \"hit_z\"          : hit_z,\n",
    "                \"KMU_endpoint_x\" : KMU_endpoint_x,\n",
    "                \"KMU_endpoint_y\" : KMU_endpoint_y,\n",
    "                \"KMU_endpoint_z\" : KMU_endpoint_z,\n",
    "                \"Charge\"         : curr_charge,\n",
    "                \"Time\"           : curr_timing\n",
    "            })\n",
    "\n",
    "    ret_df = pd.DataFrame(rows)\n",
    "    \n",
    "    ret_df['first_hit_time'] = ret_df['event_idx'].map(lambda x: event_first_hits[x][0])\n",
    "    ret_df['first_hit_strip_z'] = ret_df['event_idx'].map(lambda x: event_first_hits[x][1])\n",
    "    ret_df['first_hit_strip_x'] = ret_df['event_idx'].map(lambda x: event_first_hits[x][2])\n",
    "    return ret_df\n",
    "data = test_newer_prepare_nn_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e0a27c5-a11b-4ef4-98b2-c03c30a2edbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data in new_prepare_nn_input...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling took 13.783637285232544 seconds\n",
      "Processing signal...\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 36.0925 s\n",
      "File: /tmp/ipykernel_4051716/562588580.py\n",
      "Function: test_newer_prepare_nn_input at line 9\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     9                                           def test_newer_prepare_nn_input(processed_data = processed_data, normalizing_flow=model_compile, batch_size=50000, device='cuda',pixel_threshold = 5):\n",
      "    10         1    2098220.0    2e+06      0.0      processer = SiPMSignalProcessor()\n",
      "    11                                               \n",
      "    12         1        893.0    893.0      0.0      all_context = []\n",
      "    13         1        863.0    863.0      0.0      all_time_pixels = []\n",
      "    14         1        664.0    664.0      0.0      all_metadata = []\n",
      "    15         1       1088.0   1088.0      0.0      num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
      "    16         1     172556.0 172556.0      0.0      print(\"Processing data in new_prepare_nn_input...\")\n",
      "    17        11   23259726.0    2e+06      0.1      for event_idx, event_data in tqdm(processed_data.items()):\n",
      "    18        36      42707.0   1186.3      0.0          for stave_idx, stave_data in event_data.items():\n",
      "    19       164     112287.0    684.7      0.0              for layer_idx, layer_data in stave_data.items():\n",
      "    20      1237     648078.0    523.9      0.0                  for segment_idx, segment_data in layer_data.items():\n",
      "    21      1099     479965.0    436.7      0.0                      trueID_list = []\n",
      "    22     17178    7500648.0    436.6      0.0                      for particle_id, particle_data in segment_data.items():\n",
      "    23                                           #                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
      "    24                                           #                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
      "    25     32158  144594157.0   4496.4      0.4                          base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
      "    26     16079    3887803.0    241.8      0.0                                                      dtype=torch.float32)\n",
      "    27     32158  135893092.0   4225.8      0.4                          base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
      "    28     16079    4000871.0    248.8      0.0                                                          dtype=torch.float32)\n",
      "    29     32158  135415538.0   4210.9      0.4                          base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
      "    30     16079    4084367.0    254.0      0.0                                                          dtype=torch.float32)\n",
      "    31     16079    5985794.0    372.3      0.0                          if particle_data['trueID'] not in  trueID_list:\n",
      "    32      1099    1105911.0   1006.3      0.0                              trueID_list.append(particle_data['trueID'])\n",
      "    33     48237   29108241.0    603.4      0.1                          for SiPM_idx in range(2):\n",
      "    34     32158    8437954.0    262.4      0.0                              z_pos = particle_data['z_pos']\n",
      "    35     32158  134600654.0   4185.6      0.4                              context = base_context.clone()\n",
      "    36     32158  148810398.0   4627.5      0.4                              context[0] = z_pos\n",
      "    37     32158    9754494.0    303.3      0.0                              num_pixel_tag = num_pixel_list[SiPM_idx]\n",
      "    38     32158  214846367.0   6681.0      0.6                              all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
      "    39     32158    9972086.0    310.1      0.0                              if(SiPM_idx == 0):\n",
      "    40     16079  100682710.0   6261.8      0.3                                  all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
      "    41                                                                       else:\n",
      "    42     16079  100926698.0   6276.9      0.3                                  all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
      "    43                                                                       # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
      "    44     32158   26157853.0    813.4      0.1                              fields = [\n",
      "    45                                                                           'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
      "    46                                                                           'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
      "    47                                                                           'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
      "    48                                                                           'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
      "    49                                                                           'KMU_endpoint_y', 'KMU_endpoint_z'\n",
      "    50                                                                       ]\n",
      "    51                                           \n",
      "    52                                                                       # Print types of each particle_data field\n",
      "    53                                           #                             for field in fields:\n",
      "    54                                           #                                 value = particle_data.get(field, None)\n",
      "    55                                           #                                 print(f\"{field}: {type(value)}\")\n",
      "    56                                           \n",
      "    57                                           #                             # Print the type of len(trueID_list)\n",
      "    58                                           #                             print(f\"len(trueID_list): {type(len(trueID_list))}\")\n",
      "    59                                           \n",
      "    60     32158  113702356.0   3535.7      0.3                              all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
      "    61                                           \n",
      "    62         1   66063895.0    7e+07      0.2      all_context = torch.cat(all_context)\n",
      "    63         1   65627027.0    7e+07      0.2      all_time_pixels = torch.cat(all_time_pixels)\n",
      "    64                                               \n",
      "    65         1     151956.0 151956.0      0.0      print(\"Sampling data...\")\n",
      "    66         1       1147.0   1147.0      0.0      sampled_data = []\n",
      "    67         1       2578.0   2578.0      0.0      begin = time.time()\n",
      "    68        22   50476650.0    2e+06      0.1      for i in tqdm(range(0, len(all_context), batch_size)):\n",
      "    69        21     484749.0  23083.3      0.0          batch_end = min(i + batch_size, len(all_context))\n",
      "    70        21   30458413.0    1e+06      0.1          batch_context = all_context[i:batch_end].to(device)\n",
      "    71        21     387131.0  18434.8      0.0          batch_time_pixels = all_time_pixels[i:batch_end]\n",
      "    72                                                   \n",
      "    73        21     736074.0  35051.1      0.0          with torch.no_grad():\n",
      "    74        21        1e+10    6e+08     33.5              samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
      "    75                                                   \n",
      "    76        21 1611394341.0    8e+07      4.5          sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
      "    77         1       1621.0   1621.0      0.0      end = time.time()\n",
      "    78         1      49587.0  49587.0      0.0      print(f\"sampling took {end - begin} seconds\")\n",
      "    79         1      18348.0  18348.0      0.0      print(\"Processing signal...\")\n",
      "    80         1     590713.0 590713.0      0.0      processor = SiPMSignalProcessor()\n",
      "    81         1        874.0    874.0      0.0      rows = []\n",
      "    82         1        525.0    525.0      0.0      trueID_dict = {}\n",
      "    83         1        708.0    708.0      0.0      trueID_dict_running_idx = 0\n",
      "    84         1        526.0    526.0      0.0      event_first_hits = {}\n",
      "    85                                           \n",
      "    86                                               # Sort the data first (required for groupby)\n",
      "    87         1 2702963670.0    3e+09      7.5      sorted_data = sorted(zip(all_metadata, sampled_data), key=get_key)\n",
      "    88                                           \n",
      "    89                                               # Process each group\n",
      "    90      1018    1445692.0   1420.1      0.0      for key, group in groupby(sorted_data, key=get_key):\n",
      "    91      1017     372320.0    366.1      0.0          event_idx, stave_idx, layer_idx, segment_idx = key\n",
      "    92                                           \n",
      "    93                                                   # Initialize arrays for both SiPMs\n",
      "    94      1017    5993793.0   5893.6      0.0          sipm_samples = [[], []]\n",
      "    95                                           \n",
      "    96                                                   # Get the first metadata tuple for this group (they should all be the same within a group)\n",
      "    97      1017     722598.0    710.5      0.0          first_item = next(group)\n",
      "    98      1017     501950.0    493.6      0.0          metadata = first_item[0]\n",
      "    99      1017     780398.0    767.4      0.0          _, _, _, _, _, momentum,trueID,truePID,hitID,hitPID,theta,phi,strip_x,strip_y,strip_z,trueID_list_len,hit_x,hit_y,hit_z,KMU_trueID,KMU_truePID,KMU_true_phi,KMU_true_momentum_mag,KMU_endpoint_x,KMU_endpoint_y,KMU_endpoint_z = metadata\n",
      "   100      1017    1467908.0   1443.4      0.0          sipm_samples[first_item[0][4]].append(first_item[1])\n",
      "   101                                           \n",
      "   102                                                   # Process rest of group\n",
      "   103   1022505 1041184958.0   1018.3      2.9          for metadata, sample in group:\n",
      "   104   1021488  306634393.0    300.2      0.8              sipm_idx = metadata[4]\n",
      "   105   1021488  806657924.0    789.7      2.2              sipm_samples[sipm_idx].append(sample)\n",
      "   106                                           \n",
      "   107                                                   # Process each SiPM's samples\n",
      "   108      3051    2865439.0    939.2      0.0          for curr_SiPM_idx in range(2):\n",
      "   109      2034     891758.0    438.4      0.0              if not sipm_samples[curr_SiPM_idx]:\n",
      "   110        74      22397.0    302.7      0.0                  continue\n",
      "   111                                           \n",
      "   112      1960 8074538594.0    4e+06     22.4              photon_times = np.array(sipm_samples[curr_SiPM_idx]) * 10**(-9)\n",
      "   113      1960 7520107652.0    4e+06     20.8              time_arr, waveform = processor.generate_waveform(photon_times)\n",
      "   114      1960  260651235.0 132985.3      0.7              timing = processor.get_pulse_timing(waveform, threshold=pixel_threshold)\n",
      "   115                                           \n",
      "   116      1960     855278.0    436.4      0.0              if timing is None:\n",
      "   117       663     214545.0    323.6      0.0                  continue\n",
      "   118                                           \n",
      "   119      1297   47216249.0  36404.2      0.1              curr_charge = processor.integrate_charge(waveform) * 1e6\n",
      "   120      1297    1175563.0    906.4      0.0              curr_timing = timing * 1e8\n",
      "   121                                                       \n",
      "   122      1297    1200116.0    925.3      0.0              if event_idx not in event_first_hits or curr_timing < event_first_hits[event_idx][0]:\n",
      "   123        74      64444.0    870.9      0.0                  event_first_hits[event_idx] = (curr_timing, strip_z, strip_x)\n",
      "   124                                           \n",
      "   125                                                       # Handle trueID translation\n",
      "   126      1297     525210.0    404.9      0.0              if trueID_list_len > 1:\n",
      "   127                                                           translated_trueID = -1\n",
      "   128                                                       else:\n",
      "   129      1297     809866.0    624.4      0.0                  event_true_key = (event_idx, trueID)\n",
      "   130      1297     771618.0    594.9      0.0                  if event_true_key not in trueID_dict:\n",
      "   131        10       6135.0    613.5      0.0                      trueID_dict[event_true_key] = trueID_dict_running_idx\n",
      "   132        10       4111.0    411.1      0.0                      trueID_dict_running_idx += 1\n",
      "   133      1297     611088.0    471.2      0.0                  translated_trueID = trueID_dict[event_true_key]\n",
      "   134                                           \n",
      "   135                                                       # Create row\n",
      "   136      2594    4410508.0   1700.3      0.0              rows.append({\n",
      "   137      1297     413153.0    318.5      0.0                  \"event_idx\": event_idx,\n",
      "   138      1297     485888.0    374.6      0.0                  \"stave_idx\": stave_idx,\n",
      "   139      1297     456270.0    351.8      0.0                  \"layer_idx\": layer_idx,\n",
      "   140      1297     449577.0    346.6      0.0                  \"segment_idx\": segment_idx,\n",
      "   141      1297     437389.0    337.2      0.0                  \"SiPM_idx\": curr_SiPM_idx,\n",
      "   142      1297     414307.0    319.4      0.0                  \"trueID\": translated_trueID,\n",
      "   143      1297     515270.0    397.3      0.0                  \"truePID\": truePID,\n",
      "   144      1297     515528.0    397.5      0.0                  \"hitID\": hitID,\n",
      "   145      1297     483527.0    372.8      0.0                  \"P\"              : momentum,\n",
      "   146      1297     428165.0    330.1      0.0                  \"Theta\"          : theta,\n",
      "   147      1297     459090.0    354.0      0.0                  \"Phi\"            : phi,\n",
      "   148      1297     551338.0    425.1      0.0                  \"strip_x\"        : strip_z,\n",
      "   149      1297     472646.0    364.4      0.0                  \"strip_y\"        : strip_x,\n",
      "   150      1297     433667.0    334.4      0.0                  \"strip_z\"        : strip_y,\n",
      "   151      1297     455610.0    351.3      0.0                  \"hit_x\"          : hit_x,\n",
      "   152      1297     463997.0    357.7      0.0                  \"hit_y\"          : hit_y,\n",
      "   153      1297     455008.0    350.8      0.0                  \"hit_z\"          : hit_z,\n",
      "   154      1297     470649.0    362.9      0.0                  \"KMU_endpoint_x\" : KMU_endpoint_x,\n",
      "   155      1297     456623.0    352.1      0.0                  \"KMU_endpoint_y\" : KMU_endpoint_y,\n",
      "   156      1297     455555.0    351.2      0.0                  \"KMU_endpoint_z\" : KMU_endpoint_z,\n",
      "   157      1297     447968.0    345.4      0.0                  \"Charge\"         : curr_charge,\n",
      "   158      1297     461184.0    355.6      0.0                  \"Time\"           : curr_timing\n",
      "   159                                                       })\n",
      "   160                                           \n",
      "   161         1    8795011.0    9e+06      0.0      ret_df = pd.DataFrame(rows)\n",
      "   162                                               \n",
      "   163         1    2871208.0    3e+06      0.0      ret_df['first_hit_time'] = ret_df['event_idx'].map(lambda x: event_first_hits[x][0])\n",
      "   164         1    1577239.0    2e+06      0.0      ret_df['first_hit_strip_z'] = ret_df['event_idx'].map(lambda x: event_first_hits[x][1])\n",
      "   165         1    1399287.0    1e+06      0.0      ret_df['first_hit_strip_x'] = ret_df['event_idx'].map(lambda x: event_first_hits[x][2])\n",
      "   166         1        288.0    288.0      0.0      return ret_df\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(test_newer_prepare_nn_input)\n",
    "profiler.run('test_newer_prepare_nn_input()')\n",
    "profiler.print_stats()\n",
    "with open('profiling/Analyze_dev/test_first_hit_profile_jan_18_10events.txt', 'w') as f:\n",
    "    profiler.print_stats(stream=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce1f79-c9bb-4631-9553-17fded59f6c4",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf0431f3-51f5-4cc4-8c05-9be0589dd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiPMSignalProcessor:\n",
    "    def __init__(self, \n",
    "                 sampling_rate=40,  # 40 GHz sampling rate\n",
    "                 tau_rise=1,       # 1 ns rise time\n",
    "                 tau_fall=10,      # 50 ns fall time\n",
    "                 window=200,       # 200 ns time window\n",
    "                 cfd_delay=5,      # 5 ns delay for CFD\n",
    "                 cfd_fraction=0.3):   # 30% fraction for CFD\n",
    "        \n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.tau_rise = tau_rise\n",
    "        self.tau_fall = tau_fall\n",
    "        self.window = window\n",
    "        self.cfd_delay = cfd_delay\n",
    "        self.cfd_fraction = cfd_fraction\n",
    "        \n",
    "        # Time array for single pulse shape\n",
    "        self.time = np.arange(0, self.window, 1/self.sampling_rate)\n",
    "        \n",
    "        # Generate single pulse shape\n",
    "        self.pulse_shape = self._generate_pulse_shape()\n",
    "    \n",
    "    def _generate_pulse_shape(self):\n",
    "        \"\"\"Generate normalized pulse shape for a single photon\"\"\"\n",
    "        shape = (1 - np.exp(-self.time/self.tau_rise)) * np.exp(-self.time/self.tau_fall)\n",
    "        return shape / np.max(shape)  # Normalize\n",
    "    \n",
    "    def generate_waveform(self, photon_times):\n",
    "        \"\"\"Generate waveform from list of photon arrival times\"\"\"\n",
    "        # Initialize waveform array\n",
    "        waveform = np.zeros_like(self.time)\n",
    "        \n",
    "        # Add pulse for each photon\n",
    "        for t in photon_times:\n",
    "            if 0 <= t < self.window:\n",
    "                idx = int(t * self.sampling_rate)\n",
    "                remaining_samples = len(self.time) - idx\n",
    "                waveform[idx:] += self.pulse_shape[:remaining_samples]\n",
    "        \n",
    "        return self.time, waveform\n",
    "    \n",
    "    def integrate_charge(self, waveform, integration_start=0, integration_time=100):\n",
    "        \"\"\"Integrate charge in specified time window\"\"\"\n",
    "        start_idx = int(integration_start * self.sampling_rate)\n",
    "        end_idx = int((integration_start + integration_time) * self.sampling_rate)\n",
    "        \n",
    "        # Integrate using trapezoidal rule\n",
    "        charge = np.trapezoid(waveform[start_idx:end_idx], dx=1/self.sampling_rate)\n",
    "        return charge\n",
    "    def constant_threshold_timing(self,waveform,threshold):\n",
    "        for i in range(len(self.time)):\n",
    "            if(waveform[i] > threshold):\n",
    "                return self.time[i]\n",
    "        return -1\n",
    "        \n",
    "    def apply_cfd(self, waveform, use_interpolation=True):\n",
    "        \"\"\"Apply Constant Fraction Discrimination to the waveform.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        waveform : numpy.ndarray\n",
    "            Input waveform to process\n",
    "        use_interpolation : bool, optional\n",
    "            If True, use linear interpolation for sub-sample precision\n",
    "            If False, return the sample index of zero crossing\n",
    "            Default is True\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple (numpy.ndarray, float)\n",
    "            CFD processed waveform and the zero-crossing time in seconds.\n",
    "            If use_interpolation is False, zero-crossing time will be aligned\n",
    "            to sample boundaries.\n",
    "        \"\"\"\n",
    "        # Calculate delay in samples\n",
    "        delay_samples = int(self.cfd_delay * self.sampling_rate)\n",
    "\n",
    "        # Create delayed and attenuated versions of the waveform\n",
    "        delayed_waveform = np.pad(waveform, (delay_samples, 0))[:-delay_samples]\n",
    "        attenuated_waveform = -self.cfd_fraction * waveform\n",
    "\n",
    "        # Calculate CFD waveform\n",
    "        cfd_waveform = delayed_waveform + attenuated_waveform\n",
    "\n",
    "        # Find all zero crossings\n",
    "        zero_crossings = np.where(np.diff(np.signbit(cfd_waveform)))[0]\n",
    "\n",
    "        if len(zero_crossings) < 2:  # Need at least two crossings for valid CFD\n",
    "            return cfd_waveform, None\n",
    "\n",
    "        # Find the rising edge of the original pulse\n",
    "        pulse_start = np.where(waveform > np.max(waveform) * 0.1)[0]  # 10% threshold\n",
    "        if len(pulse_start) == 0:\n",
    "            return cfd_waveform, None\n",
    "        pulse_start = pulse_start[0]\n",
    "\n",
    "        # Find the first zero crossing that occurs after the pulse starts\n",
    "        valid_crossings = zero_crossings[zero_crossings > pulse_start]\n",
    "        if len(valid_crossings) == 0:\n",
    "            return cfd_waveform, None\n",
    "\n",
    "        crossing_idx = valid_crossings[0]\n",
    "\n",
    "        if not use_interpolation:\n",
    "            # Simply return the sample index converted to time\n",
    "            crossing_time = crossing_idx / self.sampling_rate\n",
    "        else:\n",
    "            # Use linear interpolation for sub-sample precision\n",
    "            y1 = cfd_waveform[crossing_idx]\n",
    "            y2 = cfd_waveform[crossing_idx + 1]\n",
    "\n",
    "            # Calculate fractional position of zero crossing\n",
    "            fraction = -y1 / (y2 - y1)\n",
    "\n",
    "            # Calculate precise crossing time\n",
    "            crossing_time = (crossing_idx + fraction) / self.sampling_rate\n",
    "\n",
    "        return cfd_waveform, crossing_time\n",
    "\n",
    "\n",
    "    def get_pulse_timing(self, waveform, threshold=0.1):\n",
    "        \"\"\"Get pulse timing using CFD method with additional validation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        waveform : numpy.ndarray\n",
    "            Input waveform to analyze\n",
    "        threshold : float\n",
    "            Minimum amplitude threshold for valid pulses (relative to max amplitude)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float or None\n",
    "            Timestamp of the pulse in seconds, or None if no valid pulse found\n",
    "        \"\"\"\n",
    "        # Check if pulse amplitude exceeds threshold\n",
    "        max_amplitude = np.max(waveform)\n",
    "        if max_amplitude < threshold:\n",
    "            return None\n",
    "            \n",
    "        # Apply CFD\n",
    "        _, crossing_time = self.apply_cfd(waveform)\n",
    "        \n",
    "        return crossing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "41484993-fa05-4bdd-8fb5-49aa0adfb2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing input for NF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:15<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling took 15.160471200942993 seconds\n",
      "Processing signal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data = processed_data\n",
    "normalizing_flow=model_compile\n",
    "batch_size=50000\n",
    "device='cuda'\n",
    "pixel_threshold = 5\n",
    "processer = SiPMSignalProcessor()\n",
    "\n",
    "pixel_dict = {}\n",
    "\n",
    "all_context = []\n",
    "all_time_pixels = []\n",
    "all_metadata = []\n",
    "num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
    "print(\"Preparing input for NF\")\n",
    "for event_idx, event_data in tqdm(processed_data.items()):\n",
    "    for stave_idx, stave_data in event_data.items():\n",
    "        for layer_idx, layer_data in stave_data.items():\n",
    "            for segment_idx, segment_data in layer_data.items():\n",
    "                trueID_list = []\n",
    "                for particle_id, particle_data in segment_data.items():\n",
    "#                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
    "#                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
    "                    base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
    "                                                dtype=torch.float32)\n",
    "                    base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
    "                                                    dtype=torch.float32)\n",
    "                    base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
    "                                                    dtype=torch.float32)\n",
    "                    if particle_data['trueID'] not in  trueID_list:\n",
    "                        trueID_list.append(particle_data['trueID'])\n",
    "                    for SiPM_idx in range(2):\n",
    "                        z_pos = particle_data['z_pos']\n",
    "                        context = base_context.clone()\n",
    "                        context[0] = z_pos\n",
    "                        num_pixel_tag = num_pixel_list[SiPM_idx]\n",
    "                        all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        if(SiPM_idx == 0):\n",
    "                            all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        else:\n",
    "                            all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
    "                        fields = [\n",
    "                            'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
    "                            'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
    "                            'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
    "                            'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
    "                            'KMU_endpoint_y', 'KMU_endpoint_z'\n",
    "                        ]\n",
    "\n",
    "                        all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
    "                        particle_key = (event_idx,stave_idx,layer_idx,segment_idx)\n",
    "                        if(particle_key in pixel_dict):\n",
    "                            pixel_dict[particle_key][0] +=particle_data[\"num_pixels_high_z\"]\n",
    "                            pixel_dict[particle_key][1] +=particle_data[\"num_pixels_low_z\"]\n",
    "                        else:\n",
    "                            pixel_dict[particle_key] =[particle_data[\"num_pixels_high_z\"],particle_data[\"num_pixels_low_z\"]]\n",
    "all_context = torch.cat(all_context)\n",
    "all_time_pixels = torch.cat(all_time_pixels)\n",
    "\n",
    "print(\"Sampling data...\")\n",
    "sampled_data = []\n",
    "begin = time.time()\n",
    "for i in tqdm(range(0, len(all_context), batch_size)):\n",
    "    batch_end = min(i + batch_size, len(all_context))\n",
    "    batch_context = all_context[i:batch_end].to(device)\n",
    "    batch_time_pixels = all_time_pixels[i:batch_end]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
    "\n",
    "    sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
    "end = time.time()\n",
    "print(f\"sampling took {end - begin} seconds\")\n",
    "print(\"Processing signal...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e571e3b4-6b7f-44a5-a6ee-747df41a3213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DF took 19.41303563117981 seconds\n"
     ]
    }
   ],
   "source": [
    "# VARIABLES FOR SAVING DATA AS DF\n",
    "processer = SiPMSignalProcessor()\n",
    "rows = []\n",
    "\n",
    "seen_keys = set()\n",
    "curr_key = (-1,-1,-1,-1)\n",
    "\n",
    "pixel_counter = np.zeros(2,dtype=int)\n",
    "processor = SiPMSignalProcessor()\n",
    "\n",
    "translated_trueID = 0\n",
    "trueID_dict_running_idx = 0\n",
    "trueID_dict = {}\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "#     sample_idx = 0\n",
    "for (event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, momentum,trueID,truePID,hitID,hitPID,theta,phi,strip_x,strip_y,strip_z,trueID_list_len,hit_x,hit_y,hit_z,KMU_trueID,KMU_truePID,KMU_true_phi,KMU_true_momentum_mag,KMU_endpoint_x,KMU_endpoint_y,KMU_endpoint_z), sample in zip(all_metadata, sampled_data):\n",
    "\n",
    "    #progress bar\n",
    "#         floor_percent = int(np.floor(len(sampled_data) / 100))\n",
    "#         if(sample_idx % floor_percent == 0):\n",
    "#             curr_time = time.time()\n",
    "#             print(f\"Signal Processing is now {int(np.floor(sample_idx / len(sampled_data) * 100))}% complete (time elapsed: {curr_time - begin})\")\n",
    "#             clear_output(wait = True)\n",
    "#         sample_idx += 1\n",
    "\n",
    "    # Work with all samples of one SiPM together\n",
    "    key = (event_idx, stave_idx, layer_idx, segment_idx)\n",
    "    if key in seen_keys:\n",
    "        if key == curr_key:\n",
    "            current_samples[SiPM_idx][pixel_counter[SiPM_idx]] = sample\n",
    "            pixel_counter[SiPM_idx] = pixel_counter[SiPM_idx] + 1\n",
    "        else:\n",
    "            continue\n",
    "            print(f\"ERROR: key: {key} | curr_key: {curr_key}\")\n",
    "    # First key\n",
    "    elif curr_key == (-1,-1,-1,-1):\n",
    "        current_samples = [np.empty(pixel_dict[key][0]),np.empty(pixel_dict[key][1])]\n",
    "        current_samples[SiPM_idx][pixel_counter[SiPM_idx]] = sample\n",
    "        pixel_counter[SiPM_idx] = pixel_counter[SiPM_idx] + 1\n",
    "        seen_keys.add(key)\n",
    "        curr_key = key\n",
    "    # End of curr_key: perform calc\n",
    "    else:\n",
    "        #calculate photon stuff on current_samples\n",
    "\n",
    "        '''IMPLEMENTING PREDICTION INPUT PULSE SEGMENT BY SEGMENT'''\n",
    "        curr_event_idx = curr_key[0]\n",
    "        curr_stave_idx = curr_key[1]\n",
    "        curr_layer_idx = curr_key[2]\n",
    "        curr_segment_idx = curr_key[3]\n",
    "        for curr_SiPM_idx in range(2):\n",
    "            trigger = False\n",
    "            photon_times_not_np = current_samples[curr_SiPM_idx]\n",
    "            photon_times = np.array(photon_times_not_np)\n",
    "            if(len(photon_times) > 0):\n",
    "                time_arr,waveform = processor.generate_waveform(photon_times)\n",
    "                timing = processer.get_pulse_timing(waveform,threshold = pixel_threshold)\n",
    "                if(timing is not None):\n",
    "                    #scale inputs to avoid exploding gradients\n",
    "                    curr_charge = processor.integrate_charge(waveform) / 100\n",
    "                    curr_timing = timing /10\n",
    "                    trigger = True\n",
    "                #skip segments that don't pass the threshold\n",
    "                else:\n",
    "                    continue\n",
    "            #skip segments with no photon hits\n",
    "            else:\n",
    "                continue\n",
    "            if(trueID_list_len > 1):\n",
    "                translated_trueID = -1\n",
    "            else:\n",
    "                if((event_idx,trueID) not in trueID_dict):\n",
    "                    trueID_dict[(event_idx,trueID)] = trueID_dict_running_idx\n",
    "                    trueID_dict_running_idx += 1\n",
    "                translated_trueID = trueID_dict[(event_idx,trueID)]\n",
    "            new_row = {\n",
    "                \"event_idx\"      : curr_event_idx,\n",
    "                \"stave_idx\"      : curr_stave_idx,\n",
    "                \"layer_idx\"      : curr_layer_idx,\n",
    "                \"segment_idx\"    : curr_segment_idx,\n",
    "                \"SiPM_idx\"    : curr_SiPM_idx,\n",
    "                \"trueID\"         : translated_trueID,\n",
    "                \"truePID\"        : trueID,\n",
    "                \"hitID\"          : hitID,\n",
    "                \"P\"              : momentum,\n",
    "                \"Theta\"          : theta,\n",
    "                \"Phi\"            : phi,\n",
    "                \"strip_x\"        : strip_z,\n",
    "                \"strip_y\"        : strip_x,\n",
    "                \"strip_z\"        : strip_y,\n",
    "                \"hit_x\"          : hit_x,\n",
    "                \"hit_y\"          : hit_y,\n",
    "                \"hit_z\"          : hit_z,\n",
    "                \"KMU_endpoint_x\" : KMU_endpoint_x,\n",
    "                \"KMU_endpoint_y\" : KMU_endpoint_y,\n",
    "                \"KMU_endpoint_z\" : KMU_endpoint_z,\n",
    "                \"Charge\"         : curr_charge,\n",
    "                \"Time\"           : curr_timing\n",
    "            }\n",
    "            rows.append(new_row)\n",
    "        ''' END IMPLEMENTATION '''\n",
    "        #reset current samples for new key\n",
    "        seen_keys.add(key)\n",
    "        pixel_counter = pixel_counter = np.zeros(2,dtype=int)\n",
    "        current_samples = [np.empty(pixel_dict[key][0]),np.empty(pixel_dict[key][1])]\n",
    "        current_samples[SiPM_idx][pixel_counter[SiPM_idx]] = sample\n",
    "        pixel_counter[SiPM_idx] = pixel_counter[SiPM_idx] + 1\n",
    "        curr_key = key\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "ret_df = pd.DataFrame(rows)\n",
    "print(f\"Creating DF took {end - begin} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47b6d767-bd9a-40e0-8834-7f712423a4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.64141715e-310, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "       0.00000000e+000, 6.95310027e-310, 4.94065646e-324, 4.94065646e-324,\n",
       "       6.90719599e-310, 6.95310027e-310, 1.48219694e-323, 6.32404027e-322,\n",
       "       4.94065646e-324, 6.90719585e-310, 4.94065646e-324, 0.00000000e+000,\n",
       "       0.00000000e+000, 6.95310027e-310, 0.00000000e+000, 6.90719585e-310,\n",
       "       6.95310027e-310, 6.90709997e-310, 2.12199579e-314, 6.95310027e-310,\n",
       "       2.12199585e-314, 6.95293141e-310, 2.12199579e-314, 6.90709630e-310,\n",
       "       6.95310027e-310, 6.95310027e-310, 7.63918485e-313, 4.94065646e-323,\n",
       "       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "       6.90697860e-310, 1.40107117e+001, 1.71535511e+001, 1.56047277e+001,\n",
       "       1.48282042e+001, 1.63265953e+001, 1.33217020e+001, 1.87257118e+001,\n",
       "       2.18567371e+001, 1.55207739e+001, 1.46341257e+001, 1.59433670e+001,\n",
       "       1.47265110e+001, 1.74372787e+001, 1.68182716e+001, 1.84677734e+001,\n",
       "       1.40344086e+001, 2.80669518e+001, 3.16355972e+001, 2.69963322e+001,\n",
       "       2.29423981e+001, 2.71258106e+001])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_samples[SiPM_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c327791-5549-492c-8cbd-d166ea4ce166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 66])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57809e40-c67e-47bd-b59a-cbf9d510aff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_counter[SiPM_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "596cf9ee-99bf-4000-b8df-4f1704109dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0', '1', '2', '32')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cad37071-46e0-4368-a01d-1683148a8fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SiPM_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf23a10b-c532-47e1-833d-87fa173778d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_dict[('0','1','2','32')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0df2c5a2-9ab6-47c2-8228-b205aa1ace17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_venv",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
