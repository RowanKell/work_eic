{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5e61d4-9181-4950-ba4e-0293caabed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "from io import StringIO\n",
    "from functools import wraps\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pstats\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pympler import asizeof\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "from time_res_util import get_compiled_NF_model\n",
    "from momentum_prediction_util import load_defaultdict, SiPMSignalProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453c96b8-3afc-48b2-9da5-bdd69f5c151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_function(func):\n",
    "    \"\"\"\n",
    "    Decorator to profile a specific function using cProfile\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        profiler = cProfile.Profile()\n",
    "        try:\n",
    "            return profiler.runcall(func, *args, **kwargs)\n",
    "        finally:\n",
    "            s = StringIO()\n",
    "            stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n",
    "            stats.print_stats(20)  # Print top 20 time-consuming operations\n",
    "            print(s.getvalue())\n",
    "    return wrapper\n",
    "\n",
    "'''MEMORY PROFILING'''\n",
    "import linecache\n",
    "import os\n",
    "import tracemalloc\n",
    "\n",
    "def display_top(snapshot, key_type='lineno', limit=3):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\n",
    "\n",
    "tracemalloc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4671432-d096-45c6-b89a-1091cc3f240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/vossenlab/rck32/ML_venv/lib64/python3.9/site-packages/normflows/core.py:213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "inputProcessedData = \"./data/processed_data/jan_13_new_analyze_10events.json\"\n",
    "model_compile = get_compiled_NF_model()\n",
    "processed_data = load_defaultdict(inputProcessedData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aebf93-3744-4f63-a81d-9ea33a90e2c8",
   "metadata": {},
   "source": [
    "## Current fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b862ca-5066-4057-82d9-2f334924dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newer_prepare_nn_input(processed_data = processed_data, normalizing_flow=model_compile, batch_size=50000, device='cuda',pixel_threshold = 5):\n",
    "    processer = SiPMSignalProcessor()\n",
    "    \n",
    "    all_context = []\n",
    "    all_time_pixels = []\n",
    "    all_metadata = []\n",
    "    num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
    "    print(\"Preparing input for NF\")\n",
    "    for event_idx, event_data in tqdm(processed_data.items()):\n",
    "        for stave_idx, stave_data in event_data.items():\n",
    "            for layer_idx, layer_data in stave_data.items():\n",
    "                for segment_idx, segment_data in layer_data.items():\n",
    "                    trueID_list = []\n",
    "                    for particle_id, particle_data in segment_data.items():\n",
    "#                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
    "#                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
    "                        base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
    "                                                    dtype=torch.float32)\n",
    "                        base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
    "                                                        dtype=torch.float32)\n",
    "                        base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
    "                                                        dtype=torch.float32)\n",
    "                        if particle_data['trueID'] not in  trueID_list:\n",
    "                            trueID_list.append(particle_data['trueID'])\n",
    "                        for SiPM_idx in range(2):\n",
    "                            z_pos = particle_data['z_pos']\n",
    "                            context = base_context.clone()\n",
    "                            context[0] = z_pos\n",
    "                            num_pixel_tag = num_pixel_list[SiPM_idx]\n",
    "                            all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            if(SiPM_idx == 0):\n",
    "                                all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            else:\n",
    "                                all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
    "                            fields = [\n",
    "                                'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
    "                                'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
    "                                'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
    "                                'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
    "                                'KMU_endpoint_y', 'KMU_endpoint_z'\n",
    "                            ]\n",
    "\n",
    "                            # Print types of each particle_data field\n",
    "#                             for field in fields:\n",
    "#                                 value = particle_data.get(field, None)\n",
    "#                                 print(f\"{field}: {type(value)}\")\n",
    "\n",
    "#                             # Print the type of len(trueID_list)\n",
    "#                             print(f\"len(trueID_list): {type(len(trueID_list))}\")\n",
    "\n",
    "                            all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
    "\n",
    "    all_context = torch.cat(all_context)\n",
    "    all_time_pixels = torch.cat(all_time_pixels)\n",
    "    \n",
    "    print(\"Sampling data...\")\n",
    "    sampled_data = []\n",
    "    begin = time.time()\n",
    "    for i in tqdm(range(0, len(all_context), batch_size)):\n",
    "        batch_end = min(i + batch_size, len(all_context))\n",
    "        batch_context = all_context[i:batch_end].to(device)\n",
    "        batch_time_pixels = all_time_pixels[i:batch_end]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
    "        \n",
    "        sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
    "    end = time.time()\n",
    "    print(f\"sampling took {end - begin} seconds\")\n",
    "    print(\"Processing signal...\")\n",
    "    \n",
    "    \n",
    "    # VARIABLES FOR SAVING DATA AS DF\n",
    "    processer = SiPMSignalProcessor()\n",
    "    rows = []\n",
    "\n",
    "    seen_keys = set()\n",
    "    curr_key = (-1,-1,-1,-1)\n",
    "\n",
    "    current_samples = [[],[]] \n",
    "    processor = SiPMSignalProcessor()\n",
    "\n",
    "    translated_trueID = 0\n",
    "    trueID_dict_running_idx = 0\n",
    "    trueID_dict = {}\n",
    "\n",
    "    begin = time.time()\n",
    "\n",
    "#     sample_idx = 0\n",
    "    for (event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, momentum,trueID,truePID,hitID,hitPID,theta,phi,strip_x,strip_y,strip_z,trueID_list_len,hit_x,hit_y,hit_z,KMU_trueID,KMU_truePID,KMU_true_phi,KMU_true_momentum_mag,KMU_endpoint_x,KMU_endpoint_y,KMU_endpoint_z), sample in zip(all_metadata, sampled_data):\n",
    "\n",
    "        #progress bar\n",
    "#         floor_percent = int(np.floor(len(sampled_data) / 100))\n",
    "#         if(sample_idx % floor_percent == 0):\n",
    "#             curr_time = time.time()\n",
    "#             print(f\"Signal Processing is now {int(np.floor(sample_idx / len(sampled_data) * 100))}% complete (time elapsed: {curr_time - begin})\")\n",
    "#             clear_output(wait = True)\n",
    "#         sample_idx += 1\n",
    "\n",
    "        # Work with all samples of one SiPM together\n",
    "        key = (event_idx, stave_idx, layer_idx, segment_idx)\n",
    "        if key in seen_keys:\n",
    "            if key == curr_key:\n",
    "                current_samples[SiPM_idx].append(sample)\n",
    "            else:\n",
    "                continue\n",
    "                print(f\"ERROR: key: {key} | curr_key: {curr_key}\")\n",
    "        # First key\n",
    "        elif curr_key == (-1,-1,-1,-1):\n",
    "            current_samples[SiPM_idx].append(sample)\n",
    "            seen_keys.add(key)\n",
    "            curr_key = key\n",
    "        # End of curr_key: perform calc\n",
    "        else:\n",
    "            #calculate photon stuff on current_samples\n",
    "\n",
    "            '''IMPLEMENTING PREDICTION INPUT PULSE SEGMENT BY SEGMENT'''\n",
    "            curr_event_idx = curr_key[0]\n",
    "            curr_stave_idx = curr_key[1]\n",
    "            curr_layer_idx = curr_key[2]\n",
    "            curr_segment_idx = curr_key[3]\n",
    "            for curr_SiPM_idx in range(2):\n",
    "                trigger = False\n",
    "                photon_times_not_np = current_samples[curr_SiPM_idx]\n",
    "                photon_times = np.array(photon_times_not_np)\n",
    "                if(len(photon_times) > 0):\n",
    "                    time_arr,waveform = processor.generate_waveform(photon_times)\n",
    "                    timing = processer.get_pulse_timing(waveform,threshold = pixel_threshold)\n",
    "                    if(timing is not None):\n",
    "                        #scale inputs to avoid exploding gradients\n",
    "                        curr_charge = processor.integrate_charge(waveform) / 100\n",
    "                        curr_timing = timing /10\n",
    "                        trigger = True\n",
    "                    #skip segments that don't pass the threshold\n",
    "                    else:\n",
    "                        continue\n",
    "                #skip segments with no photon hits\n",
    "                else:\n",
    "                    continue\n",
    "                if(trueID_list_len > 1):\n",
    "                    translated_trueID = -1\n",
    "                else:\n",
    "                    if((event_idx,trueID) not in trueID_dict):\n",
    "                        trueID_dict[(event_idx,trueID)] = trueID_dict_running_idx\n",
    "                        trueID_dict_running_idx += 1\n",
    "                    translated_trueID = trueID_dict[(event_idx,trueID)]\n",
    "                new_row = {\n",
    "                    \"event_idx\"      : curr_event_idx,\n",
    "                    \"stave_idx\"      : curr_stave_idx,\n",
    "                    \"layer_idx\"      : curr_layer_idx,\n",
    "                    \"segment_idx\"    : curr_segment_idx,\n",
    "                    \"SiPM_idx\"    : curr_SiPM_idx,\n",
    "                    \"trueID\"         : translated_trueID,\n",
    "                    \"truePID\"        : trueID,\n",
    "                    \"hitID\"          : hitID,\n",
    "                    \"P\"              : momentum,\n",
    "                    \"Theta\"          : theta,\n",
    "                    \"Phi\"            : phi,\n",
    "                    \"strip_x\"        : strip_z,\n",
    "                    \"strip_y\"        : strip_x,\n",
    "                    \"strip_z\"        : strip_y,\n",
    "                    \"hit_x\"          : hit_x,\n",
    "                    \"hit_y\"          : hit_y,\n",
    "                    \"hit_z\"          : hit_z,\n",
    "                    \"KMU_endpoint_x\" : KMU_endpoint_x,\n",
    "                    \"KMU_endpoint_y\" : KMU_endpoint_y,\n",
    "                    \"KMU_endpoint_z\" : KMU_endpoint_z,\n",
    "                    \"Charge\"         : curr_charge,\n",
    "                    \"Time\"           : curr_timing\n",
    "                }\n",
    "                rows.append(new_row)\n",
    "            ''' END IMPLEMENTATION '''\n",
    "            #reset current samples for new key\n",
    "            seen_keys.add(key)\n",
    "            current_samples = [[],[]]\n",
    "            current_samples.append(sample)\n",
    "            curr_key = key\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    ret_df = pd.DataFrame(rows)\n",
    "    print(f\"Creating DF took {end - begin} seconds\")\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf77f3-f7b4-4a25-9b5f-6f5ae727acb9",
   "metadata": {},
   "source": [
    "## Test: pre-allocate arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "247da824-ce18-400a-957b-ccf8352c4b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_newer_prepare_nn_input(processed_data = processed_data, normalizing_flow=model_compile, batch_size=50000, device='cuda',pixel_threshold = 5):\n",
    "    processer = SiPMSignalProcessor()\n",
    "\n",
    "    pixel_dict = {}\n",
    "\n",
    "    all_context = []\n",
    "    all_time_pixels = []\n",
    "    all_metadata = []\n",
    "    num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
    "    print(\"Preparing input for NF\")\n",
    "    for event_idx, event_data in tqdm(processed_data.items()):\n",
    "        for stave_idx, stave_data in event_data.items():\n",
    "            for layer_idx, layer_data in stave_data.items():\n",
    "                for segment_idx, segment_data in layer_data.items():\n",
    "                    trueID_list = []\n",
    "                    for particle_id, particle_data in segment_data.items():\n",
    "    #                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
    "    #                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
    "                        base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
    "                                                    dtype=torch.float32)\n",
    "                        base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
    "                                                        dtype=torch.float32)\n",
    "                        base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
    "                                                        dtype=torch.float32)\n",
    "                        if particle_data['trueID'] not in  trueID_list:\n",
    "                            trueID_list.append(particle_data['trueID'])\n",
    "                        for SiPM_idx in range(2):\n",
    "                            z_pos = particle_data['z_pos']\n",
    "                            context = base_context.clone()\n",
    "                            context[0] = z_pos\n",
    "                            num_pixel_tag = num_pixel_list[SiPM_idx]\n",
    "                            all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            if(SiPM_idx == 0):\n",
    "                                all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            else:\n",
    "                                all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
    "                            fields = [\n",
    "                                'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
    "                                'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
    "                                'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
    "                                'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
    "                                'KMU_endpoint_y', 'KMU_endpoint_z'\n",
    "                            ]\n",
    "\n",
    "                            all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
    "                            particle_key = (event_idx,stave_idx,layer_idx,segment_idx)\n",
    "                            if(particle_key in pixel_dict):\n",
    "                                pixel_dict[particle_key][0] +=particle_data[\"num_pixels_high_z\"]\n",
    "                                pixel_dict[particle_key][1] +=particle_data[\"num_pixels_low_z\"]\n",
    "                            else:\n",
    "                                pixel_dict[particle_key] =[particle_data[\"num_pixels_high_z\"],particle_data[\"num_pixels_low_z\"]]\n",
    "    all_context = torch.cat(all_context)\n",
    "    all_time_pixels = torch.cat(all_time_pixels)\n",
    "\n",
    "    print(\"Sampling data...\")\n",
    "    sampled_data = []\n",
    "    begin = time.time()\n",
    "    for i in tqdm(range(0, len(all_context), batch_size)):\n",
    "        batch_end = min(i + batch_size, len(all_context))\n",
    "        batch_context = all_context[i:batch_end].to(device)\n",
    "        batch_time_pixels = all_time_pixels[i:batch_end]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
    "\n",
    "        sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
    "    end = time.time()\n",
    "    print(f\"sampling took {end - begin} seconds\")\n",
    "    print(\"Processing signal...\")\n",
    "    \n",
    "    \n",
    "    # VARIABLES FOR SAVING DATA AS DF\n",
    "    processer = SiPMSignalProcessor()\n",
    "    rows = []\n",
    "\n",
    "    seen_keys = set()\n",
    "    curr_key = (-1,-1,-1,-1)\n",
    "\n",
    "    pixel_counter = np.zeros(2,dtype=int)\n",
    "    processor = SiPMSignalProcessor()\n",
    "\n",
    "    translated_trueID = 0\n",
    "    trueID_dict_running_idx = 0\n",
    "    trueID_dict = {}\n",
    "\n",
    "    begin = time.time()\n",
    "\n",
    "    #     sample_idx = 0\n",
    "    for (event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, momentum,trueID,truePID,hitID,hitPID,theta,phi,strip_x,strip_y,strip_z,trueID_list_len,hit_x,hit_y,hit_z,KMU_trueID,KMU_truePID,KMU_true_phi,KMU_true_momentum_mag,KMU_endpoint_x,KMU_endpoint_y,KMU_endpoint_z), sample in zip(all_metadata, sampled_data):\n",
    "\n",
    "        #progress bar\n",
    "    #         floor_percent = int(np.floor(len(sampled_data) / 100))\n",
    "    #         if(sample_idx % floor_percent == 0):\n",
    "    #             curr_time = time.time()\n",
    "    #             print(f\"Signal Processing is now {int(np.floor(sample_idx / len(sampled_data) * 100))}% complete (time elapsed: {curr_time - begin})\")\n",
    "    #             clear_output(wait = True)\n",
    "    #         sample_idx += 1\n",
    "\n",
    "        # Work with all samples of one SiPM together\n",
    "        key = (event_idx, stave_idx, layer_idx, segment_idx)\n",
    "        if key in seen_keys:\n",
    "            if key == curr_key:\n",
    "                current_samples[SiPM_idx][pixel_counter[SiPM_idx]] = sample\n",
    "                pixel_counter[SiPM_idx] = pixel_counter[SiPM_idx] + 1\n",
    "            else:\n",
    "                continue\n",
    "                print(f\"ERROR: key: {key} | curr_key: {curr_key}\")\n",
    "        # First key\n",
    "        elif curr_key == (-1,-1,-1,-1):\n",
    "            current_samples = [np.empty(pixel_dict[key][0]),np.empty(pixel_dict[key][1])]\n",
    "            current_samples[SiPM_idx][pixel_counter[SiPM_idx]] = sample\n",
    "            pixel_counter[SiPM_idx] = pixel_counter[SiPM_idx] + 1\n",
    "            seen_keys.add(key)\n",
    "            curr_key = key\n",
    "        # End of curr_key: perform calc\n",
    "        else:\n",
    "            #calculate photon stuff on current_samples\n",
    "\n",
    "            '''IMPLEMENTING PREDICTION INPUT PULSE SEGMENT BY SEGMENT'''\n",
    "            curr_event_idx = curr_key[0]\n",
    "            curr_stave_idx = curr_key[1]\n",
    "            curr_layer_idx = curr_key[2]\n",
    "            curr_segment_idx = curr_key[3]\n",
    "            for curr_SiPM_idx in range(2):\n",
    "                trigger = False\n",
    "                photon_times_not_np = current_samples[curr_SiPM_idx]\n",
    "                photon_times = np.array(photon_times_not_np)\n",
    "                if(len(photon_times) > 0):\n",
    "                    time_arr,waveform = processor.generate_waveform(photon_times)\n",
    "                    timing = processer.get_pulse_timing(waveform,threshold = pixel_threshold)\n",
    "                    if(timing is not None):\n",
    "                        #scale inputs to avoid exploding gradients\n",
    "                        curr_charge = processor.integrate_charge(waveform) / 100\n",
    "                        curr_timing = timing /10\n",
    "                        trigger = True\n",
    "                    #skip segments that don't pass the threshold\n",
    "                    else:\n",
    "                        continue\n",
    "                #skip segments with no photon hits\n",
    "                else:\n",
    "                    continue\n",
    "                if(trueID_list_len > 1):\n",
    "                    translated_trueID = -1\n",
    "                else:\n",
    "                    if((event_idx,trueID) not in trueID_dict):\n",
    "                        trueID_dict[(event_idx,trueID)] = trueID_dict_running_idx\n",
    "                        trueID_dict_running_idx += 1\n",
    "                    translated_trueID = trueID_dict[(event_idx,trueID)]\n",
    "                new_row = {\n",
    "                    \"event_idx\"      : curr_event_idx,\n",
    "                    \"stave_idx\"      : curr_stave_idx,\n",
    "                    \"layer_idx\"      : curr_layer_idx,\n",
    "                    \"segment_idx\"    : curr_segment_idx,\n",
    "                    \"SiPM_idx\"    : curr_SiPM_idx,\n",
    "                    \"trueID\"         : translated_trueID,\n",
    "                    \"truePID\"        : trueID,\n",
    "                    \"hitID\"          : hitID,\n",
    "                    \"P\"              : momentum,\n",
    "                    \"Theta\"          : theta,\n",
    "                    \"Phi\"            : phi,\n",
    "                    \"strip_x\"        : strip_z,\n",
    "                    \"strip_y\"        : strip_x,\n",
    "                    \"strip_z\"        : strip_y,\n",
    "                    \"hit_x\"          : hit_x,\n",
    "                    \"hit_y\"          : hit_y,\n",
    "                    \"hit_z\"          : hit_z,\n",
    "                    \"KMU_endpoint_x\" : KMU_endpoint_x,\n",
    "                    \"KMU_endpoint_y\" : KMU_endpoint_y,\n",
    "                    \"KMU_endpoint_z\" : KMU_endpoint_z,\n",
    "                    \"Charge\"         : curr_charge,\n",
    "                    \"Time\"           : curr_timing\n",
    "                }\n",
    "                rows.append(new_row)\n",
    "            ''' END IMPLEMENTATION '''\n",
    "            #reset current samples for new key\n",
    "            seen_keys.add(key)\n",
    "            pixel_counter = pixel_counter = np.zeros(2,dtype=int)\n",
    "            current_samples = [np.empty(pixel_dict[key][0]),np.empty(pixel_dict[key][1])]\n",
    "            current_samples[SiPM_idx][pixel_counter[SiPM_idx]] = sample\n",
    "            pixel_counter[SiPM_idx] = pixel_counter[SiPM_idx] + 1\n",
    "            curr_key = key\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    ret_df = pd.DataFrame(rows)\n",
    "    print(f\"Creating DF took {end - begin} seconds\")\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e0a27c5-a11b-4ef4-98b2-c03c30a2edbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing input for NF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:15<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling took 15.419981956481934 seconds\n",
      "Processing signal...\n",
      "Creating DF took 28.556469202041626 seconds\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 42.8956 s\n",
      "File: /tmp/ipykernel_3759123/1629340420.py\n",
      "Function: newer_prepare_nn_input at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def newer_prepare_nn_input(processed_data = processed_data, normalizing_flow=model_compile, batch_size=50000, device='cuda',pixel_threshold = 5):\n",
      "     2         1     721444.0 721444.0      0.0      processer = SiPMSignalProcessor()\n",
      "     3                                               \n",
      "     4         1       1552.0   1552.0      0.0      all_context = []\n",
      "     5         1       1241.0   1241.0      0.0      all_time_pixels = []\n",
      "     6         1        505.0    505.0      0.0      all_metadata = []\n",
      "     7         1       1355.0   1355.0      0.0      num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
      "     8         1     143013.0 143013.0      0.0      print(\"Preparing input for NF\")\n",
      "     9        11   21909955.0    2e+06      0.1      for event_idx, event_data in tqdm(processed_data.items()):\n",
      "    10        36      68994.0   1916.5      0.0          for stave_idx, stave_data in event_data.items():\n",
      "    11       164     188301.0   1148.2      0.0              for layer_idx, layer_data in stave_data.items():\n",
      "    12      1237    1026686.0    830.0      0.0                  for segment_idx, segment_data in layer_data.items():\n",
      "    13      1099     531276.0    483.4      0.0                      trueID_list = []\n",
      "    14     17178   10079775.0    586.8      0.0                      for particle_id, particle_data in segment_data.items():\n",
      "    15                                           #                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
      "    16                                           #                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
      "    17     32158  161896168.0   5034.4      0.4                          base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
      "    18     16079    4056243.0    252.3      0.0                                                      dtype=torch.float32)\n",
      "    19     32158  149799181.0   4658.2      0.3                          base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
      "    20     16079    4263017.0    265.1      0.0                                                          dtype=torch.float32)\n",
      "    21     32158  147949775.0   4600.7      0.3                          base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
      "    22     16079    4304876.0    267.7      0.0                                                          dtype=torch.float32)\n",
      "    23     16079    6220706.0    386.9      0.0                          if particle_data['trueID'] not in  trueID_list:\n",
      "    24      1099    1267002.0   1152.9      0.0                              trueID_list.append(particle_data['trueID'])\n",
      "    25     48237   31652397.0    656.2      0.1                          for SiPM_idx in range(2):\n",
      "    26     32158    9461786.0    294.2      0.0                              z_pos = particle_data['z_pos']\n",
      "    27     32158  150514402.0   4680.5      0.4                              context = base_context.clone()\n",
      "    28     32158  159523198.0   4960.6      0.4                              context[0] = z_pos\n",
      "    29     32158   10814509.0    336.3      0.0                              num_pixel_tag = num_pixel_list[SiPM_idx]\n",
      "    30     32158  255549599.0   7946.7      0.6                              all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
      "    31     32158   10874697.0    338.2      0.0                              if(SiPM_idx == 0):\n",
      "    32     16079  117829690.0   7328.2      0.3                                  all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
      "    33                                                                       else:\n",
      "    34     16079  117943091.0   7335.2      0.3                                  all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
      "    35                                                                       # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
      "    36     32158   28723616.0    893.2      0.1                              fields = [\n",
      "    37                                                                           'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
      "    38                                                                           'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
      "    39                                                                           'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
      "    40                                                                           'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
      "    41                                                                           'KMU_endpoint_y', 'KMU_endpoint_z'\n",
      "    42                                                                       ]\n",
      "    43                                           \n",
      "    44                                                                       # Print types of each particle_data field\n",
      "    45                                           #                             for field in fields:\n",
      "    46                                           #                                 value = particle_data.get(field, None)\n",
      "    47                                           #                                 print(f\"{field}: {type(value)}\")\n",
      "    48                                           \n",
      "    49                                           #                             # Print the type of len(trueID_list)\n",
      "    50                                           #                             print(f\"len(trueID_list): {type(len(trueID_list))}\")\n",
      "    51                                           \n",
      "    52     32158  134513820.0   4182.9      0.3                              all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
      "    53                                           \n",
      "    54         1  117902393.0    1e+08      0.3      all_context = torch.cat(all_context)\n",
      "    55         1  113710042.0    1e+08      0.3      all_time_pixels = torch.cat(all_time_pixels)\n",
      "    56                                               \n",
      "    57         1     236835.0 236835.0      0.0      print(\"Sampling data...\")\n",
      "    58         1       1239.0   1239.0      0.0      sampled_data = []\n",
      "    59         1       3355.0   3355.0      0.0      begin = time.time()\n",
      "    60        22   51302349.0    2e+06      0.1      for i in tqdm(range(0, len(all_context), batch_size)):\n",
      "    61        21     853852.0  40659.6      0.0          batch_end = min(i + batch_size, len(all_context))\n",
      "    62        21   30382927.0    1e+06      0.1          batch_context = all_context[i:batch_end].to(device)\n",
      "    63        21     397758.0  18940.9      0.0          batch_time_pixels = all_time_pixels[i:batch_end]\n",
      "    64                                                   \n",
      "    65        21    1040649.0  49554.7      0.0          with torch.no_grad():\n",
      "    66        21        1e+10    6e+08     29.4              samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
      "    67                                                   \n",
      "    68        21 2711558789.0    1e+08      6.3          sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
      "    69         1       3600.0   3600.0      0.0      end = time.time()\n",
      "    70         1      69521.0  69521.0      0.0      print(f\"sampling took {end - begin} seconds\")\n",
      "    71         1      22594.0  22594.0      0.0      print(\"Processing signal...\")\n",
      "    72                                               \n",
      "    73                                               \n",
      "    74                                               # VARIABLES FOR SAVING DATA AS DF\n",
      "    75         1    1028198.0    1e+06      0.0      processer = SiPMSignalProcessor()\n",
      "    76         1       1223.0   1223.0      0.0      rows = []\n",
      "    77                                           \n",
      "    78         1       3355.0   3355.0      0.0      seen_keys = set()\n",
      "    79         1       1909.0   1909.0      0.0      curr_key = (-1,-1,-1,-1)\n",
      "    80                                           \n",
      "    81         1       2730.0   2730.0      0.0      current_samples = [[],[]] \n",
      "    82         1     133320.0 133320.0      0.0      processor = SiPMSignalProcessor()\n",
      "    83                                           \n",
      "    84         1        415.0    415.0      0.0      translated_trueID = 0\n",
      "    85         1        546.0    546.0      0.0      trueID_dict_running_idx = 0\n",
      "    86         1        689.0    689.0      0.0      trueID_dict = {}\n",
      "    87                                           \n",
      "    88         1       2441.0   2441.0      0.0      begin = time.time()\n",
      "    89                                           \n",
      "    90                                           #     sample_idx = 0\n",
      "    91   1022506  562522652.0    550.1      1.3      for (event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, momentum,trueID,truePID,hitID,hitPID,theta,phi,strip_x,strip_y,strip_z,trueID_list_len,hit_x,hit_y,hit_z,KMU_trueID,KMU_truePID,KMU_true_phi,KMU_true_momentum_mag,KMU_endpoint_x,KMU_endpoint_y,KMU_endpoint_z), sample in zip(all_metadata, sampled_data):\n",
      "    92                                           \n",
      "    93                                                   #progress bar\n",
      "    94                                           #         floor_percent = int(np.floor(len(sampled_data) / 100))\n",
      "    95                                           #         if(sample_idx % floor_percent == 0):\n",
      "    96                                           #             curr_time = time.time()\n",
      "    97                                           #             print(f\"Signal Processing is now {int(np.floor(sample_idx / len(sampled_data) * 100))}% complete (time elapsed: {curr_time - begin})\")\n",
      "    98                                           #             clear_output(wait = True)\n",
      "    99                                           #         sample_idx += 1\n",
      "   100                                           \n",
      "   101                                                   # Work with all samples of one SiPM together\n",
      "   102   1022505  508924052.0    497.7      1.2          key = (event_idx, stave_idx, layer_idx, segment_idx)\n",
      "   103   1022505  334944071.0    327.6      0.8          if key in seen_keys:\n",
      "   104   1021488  311012639.0    304.5      0.7              if key == curr_key:\n",
      "   105   1021488  923326265.0    903.9      2.2                  current_samples[SiPM_idx].append(sample)\n",
      "   106                                                       else:\n",
      "   107                                                           continue\n",
      "   108                                                           print(f\"ERROR: key: {key} | curr_key: {curr_key}\")\n",
      "   109                                                   # First key\n",
      "   110      1017     888190.0    873.3      0.0          elif curr_key == (-1,-1,-1,-1):\n",
      "   111         1       2373.0   2373.0      0.0              current_samples[SiPM_idx].append(sample)\n",
      "   112         1       1880.0   1880.0      0.0              seen_keys.add(key)\n",
      "   113         1        485.0    485.0      0.0              curr_key = key\n",
      "   114                                                   # End of curr_key: perform calc\n",
      "   115                                                   else:\n",
      "   116                                                       #calculate photon stuff on current_samples\n",
      "   117                                           \n",
      "   118                                                       '''IMPLEMENTING PREDICTION INPUT PULSE SEGMENT BY SEGMENT'''\n",
      "   119      1016     473762.0    466.3      0.0              curr_event_idx = curr_key[0]\n",
      "   120      1016     436608.0    429.7      0.0              curr_stave_idx = curr_key[1]\n",
      "   121      1016     364558.0    358.8      0.0              curr_layer_idx = curr_key[2]\n",
      "   122      1016     371553.0    365.7      0.0              curr_segment_idx = curr_key[3]\n",
      "   123      3048    3698406.0   1213.4      0.0              for curr_SiPM_idx in range(2):\n",
      "   124      2032     766431.0    377.2      0.0                  trigger = False\n",
      "   125      2032    4923225.0   2422.8      0.0                  photon_times_not_np = current_samples[curr_SiPM_idx]\n",
      "   126      2032        1e+10    5e+06     24.4                  photon_times = np.array(photon_times_not_np)\n",
      "   127      2032    2976888.0   1465.0      0.0                  if(len(photon_times) > 0):\n",
      "   128      1877        1e+10    6e+06     28.2                      time_arr,waveform = processor.generate_waveform(photon_times)\n",
      "   129      1877  362757809.0 193264.7      0.8                      timing = processer.get_pulse_timing(waveform,threshold = pixel_threshold)\n",
      "   130      1877    1094821.0    583.3      0.0                      if(timing is not None):\n",
      "   131                                                                   #scale inputs to avoid exploding gradients\n",
      "   132      1272   57829543.0  45463.5      0.1                          curr_charge = processor.integrate_charge(waveform) / 100\n",
      "   133      1272    1444785.0   1135.8      0.0                          curr_timing = timing /10\n",
      "   134      1272     645822.0    507.7      0.0                          trigger = True\n",
      "   135                                                               #skip segments that don't pass the threshold\n",
      "   136                                                               else:\n",
      "   137                                                                   continue\n",
      "   138                                                           #skip segments with no photon hits\n",
      "   139                                                           else:\n",
      "   140                                                               continue\n",
      "   141      1272     564739.0    444.0      0.0                  if(trueID_list_len > 1):\n",
      "   142                                                               translated_trueID = -1\n",
      "   143                                                           else:\n",
      "   144      1272    1877242.0   1475.8      0.0                      if((event_idx,trueID) not in trueID_dict):\n",
      "   145        10      11858.0   1185.8      0.0                          trueID_dict[(event_idx,trueID)] = trueID_dict_running_idx\n",
      "   146        10       5164.0    516.4      0.0                          trueID_dict_running_idx += 1\n",
      "   147      1272    1122459.0    882.4      0.0                      translated_trueID = trueID_dict[(event_idx,trueID)]\n",
      "   148      1272    4781035.0   3758.7      0.0                  new_row = {\n",
      "   149      1272     677557.0    532.7      0.0                      \"event_idx\"      : curr_event_idx,\n",
      "   150      1272     608898.0    478.7      0.0                      \"stave_idx\"      : curr_stave_idx,\n",
      "   151      1272     724410.0    569.5      0.0                      \"layer_idx\"      : curr_layer_idx,\n",
      "   152      1272     535932.0    421.3      0.0                      \"segment_idx\"    : curr_segment_idx,\n",
      "   153      1272     490391.0    385.5      0.0                      \"SiPM_idx\"    : curr_SiPM_idx,\n",
      "   154      1272     470829.0    370.1      0.0                      \"trueID\"         : translated_trueID,\n",
      "   155      1272     484990.0    381.3      0.0                      \"truePID\"        : trueID,\n",
      "   156      1272     704792.0    554.1      0.0                      \"hitID\"          : hitID,\n",
      "   157      1272     533886.0    419.7      0.0                      \"P\"              : momentum,\n",
      "   158      1272     533481.0    419.4      0.0                      \"Theta\"          : theta,\n",
      "   159      1272     689295.0    541.9      0.0                      \"Phi\"            : phi,\n",
      "   160      1272     524256.0    412.2      0.0                      \"strip_x\"        : strip_z,\n",
      "   161      1272     499268.0    392.5      0.0                      \"strip_y\"        : strip_x,\n",
      "   162      1272     441666.0    347.2      0.0                      \"strip_z\"        : strip_y,\n",
      "   163      1272     467408.0    367.5      0.0                      \"hit_x\"          : hit_x,\n",
      "   164      1272     493649.0    388.1      0.0                      \"hit_y\"          : hit_y,\n",
      "   165      1272     539273.0    424.0      0.0                      \"hit_z\"          : hit_z,\n",
      "   166      1272     528188.0    415.2      0.0                      \"KMU_endpoint_x\" : KMU_endpoint_x,\n",
      "   167      1272     476052.0    374.3      0.0                      \"KMU_endpoint_y\" : KMU_endpoint_y,\n",
      "   168      1272     477474.0    375.4      0.0                      \"KMU_endpoint_z\" : KMU_endpoint_z,\n",
      "   169      1272     457881.0    360.0      0.0                      \"Charge\"         : curr_charge,\n",
      "   170      1272     483298.0    380.0      0.0                      \"Time\"           : curr_timing\n",
      "   171                                                           }\n",
      "   172      1272    1651545.0   1298.4      0.0                  rows.append(new_row)\n",
      "   173                                                       ''' END IMPLEMENTATION '''\n",
      "   174                                                       #reset current samples for new key\n",
      "   175      1016    1678740.0   1652.3      0.0              seen_keys.add(key)\n",
      "   176      1016    7468464.0   7350.9      0.0              current_samples = [[],[]]\n",
      "   177      1016    1493183.0   1469.7      0.0              current_samples.append(sample)\n",
      "   178      1016     370200.0    364.4      0.0              curr_key = key\n",
      "   179                                           \n",
      "   180                                           \n",
      "   181         1       4062.0   4062.0      0.0      end = time.time()\n",
      "   182         1   10789082.0    1e+07      0.0      ret_df = pd.DataFrame(rows)\n",
      "   183         1     190030.0 190030.0      0.0      print(f\"Creating DF took {end - begin} seconds\")\n",
      "   184         1        276.0    276.0      0.0      return ret_df\n",
      "\n",
      "Total time: 9.46784 s\n",
      "File: /tmp/ipykernel_3759123/3869831568.py\n",
      "Function: generate_waveform at line 28\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    28                                               def generate_waveform(self, photon_times):\n",
      "    29                                                   \"\"\"Generate waveform from list of photon arrival times\"\"\"\n",
      "    30                                                   # Initialize waveform array\n",
      "    31      1877  173154311.0  92250.6      1.8          waveform = np.zeros_like(self.time)\n",
      "    32                                                   \n",
      "    33                                                   # Add pulse for each photon\n",
      "    34   1023363  684384829.0    668.8      7.2          for t in photon_times:\n",
      "    35   1021486  728892145.0    713.6      7.7              if 0 <= t < self.window:\n",
      "    36    957057 1415038257.0   1478.5     14.9                  idx = int(t * self.sampling_rate)\n",
      "    37    957057 1038855189.0   1085.5     11.0                  remaining_samples = len(self.time) - idx\n",
      "    38    957057 5426459627.0   5669.9     57.3                  waveform[idx:] += self.pulse_shape[:remaining_samples]\n",
      "    39                                                   \n",
      "    40      1877    1057511.0    563.4      0.0          return self.time, waveform\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(newer_prepare_nn_input)\n",
    "profiler.add_function(processor.generate_waveform)\n",
    "profiler.run('newer_prepare_nn_input()')\n",
    "profiler.print_stats()\n",
    "with open('profiling/Analyze_dev/profile_stats_w_waveform_jan_16_10events.txt', 'w') as f:\n",
    "    profiler.print_stats(stream=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce1f79-c9bb-4631-9553-17fded59f6c4",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf0431f3-51f5-4cc4-8c05-9be0589dd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiPMSignalProcessor:\n",
    "    def __init__(self, \n",
    "                 sampling_rate=40,  # 40 GHz sampling rate\n",
    "                 tau_rise=1,       # 1 ns rise time\n",
    "                 tau_fall=10,      # 50 ns fall time\n",
    "                 window=200,       # 200 ns time window\n",
    "                 cfd_delay=5,      # 5 ns delay for CFD\n",
    "                 cfd_fraction=0.3):   # 30% fraction for CFD\n",
    "        \n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.tau_rise = tau_rise\n",
    "        self.tau_fall = tau_fall\n",
    "        self.window = window\n",
    "        self.cfd_delay = cfd_delay\n",
    "        self.cfd_fraction = cfd_fraction\n",
    "        \n",
    "        # Time array for single pulse shape\n",
    "        self.time = np.arange(0, self.window, 1/self.sampling_rate)\n",
    "        \n",
    "        # Generate single pulse shape\n",
    "        self.pulse_shape = self._generate_pulse_shape()\n",
    "    \n",
    "    def _generate_pulse_shape(self):\n",
    "        \"\"\"Generate normalized pulse shape for a single photon\"\"\"\n",
    "        shape = (1 - np.exp(-self.time/self.tau_rise)) * np.exp(-self.time/self.tau_fall)\n",
    "        return shape / np.max(shape)  # Normalize\n",
    "    \n",
    "    def generate_waveform(self, photon_times):\n",
    "        \"\"\"Generate waveform from list of photon arrival times\"\"\"\n",
    "        # Initialize waveform array\n",
    "        waveform = np.zeros_like(self.time)\n",
    "        \n",
    "        # Add pulse for each photon\n",
    "        for t in photon_times:\n",
    "            if 0 <= t < self.window:\n",
    "                idx = int(t * self.sampling_rate)\n",
    "                remaining_samples = len(self.time) - idx\n",
    "                waveform[idx:] += self.pulse_shape[:remaining_samples]\n",
    "        \n",
    "        return self.time, waveform\n",
    "    \n",
    "    def integrate_charge(self, waveform, integration_start=0, integration_time=100):\n",
    "        \"\"\"Integrate charge in specified time window\"\"\"\n",
    "        start_idx = int(integration_start * self.sampling_rate)\n",
    "        end_idx = int((integration_start + integration_time) * self.sampling_rate)\n",
    "        \n",
    "        # Integrate using trapezoidal rule\n",
    "        charge = np.trapezoid(waveform[start_idx:end_idx], dx=1/self.sampling_rate)\n",
    "        return charge\n",
    "    def constant_threshold_timing(self,waveform,threshold):\n",
    "        for i in range(len(self.time)):\n",
    "            if(waveform[i] > threshold):\n",
    "                return self.time[i]\n",
    "        return -1\n",
    "        \n",
    "    def apply_cfd(self, waveform, use_interpolation=True):\n",
    "        \"\"\"Apply Constant Fraction Discrimination to the waveform.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        waveform : numpy.ndarray\n",
    "            Input waveform to process\n",
    "        use_interpolation : bool, optional\n",
    "            If True, use linear interpolation for sub-sample precision\n",
    "            If False, return the sample index of zero crossing\n",
    "            Default is True\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple (numpy.ndarray, float)\n",
    "            CFD processed waveform and the zero-crossing time in seconds.\n",
    "            If use_interpolation is False, zero-crossing time will be aligned\n",
    "            to sample boundaries.\n",
    "        \"\"\"\n",
    "        # Calculate delay in samples\n",
    "        delay_samples = int(self.cfd_delay * self.sampling_rate)\n",
    "\n",
    "        # Create delayed and attenuated versions of the waveform\n",
    "        delayed_waveform = np.pad(waveform, (delay_samples, 0))[:-delay_samples]\n",
    "        attenuated_waveform = -self.cfd_fraction * waveform\n",
    "\n",
    "        # Calculate CFD waveform\n",
    "        cfd_waveform = delayed_waveform + attenuated_waveform\n",
    "\n",
    "        # Find all zero crossings\n",
    "        zero_crossings = np.where(np.diff(np.signbit(cfd_waveform)))[0]\n",
    "\n",
    "        if len(zero_crossings) < 2:  # Need at least two crossings for valid CFD\n",
    "            return cfd_waveform, None\n",
    "\n",
    "        # Find the rising edge of the original pulse\n",
    "        pulse_start = np.where(waveform > np.max(waveform) * 0.1)[0]  # 10% threshold\n",
    "        if len(pulse_start) == 0:\n",
    "            return cfd_waveform, None\n",
    "        pulse_start = pulse_start[0]\n",
    "\n",
    "        # Find the first zero crossing that occurs after the pulse starts\n",
    "        valid_crossings = zero_crossings[zero_crossings > pulse_start]\n",
    "        if len(valid_crossings) == 0:\n",
    "            return cfd_waveform, None\n",
    "\n",
    "        crossing_idx = valid_crossings[0]\n",
    "\n",
    "        if not use_interpolation:\n",
    "            # Simply return the sample index converted to time\n",
    "            crossing_time = crossing_idx / self.sampling_rate\n",
    "        else:\n",
    "            # Use linear interpolation for sub-sample precision\n",
    "            y1 = cfd_waveform[crossing_idx]\n",
    "            y2 = cfd_waveform[crossing_idx + 1]\n",
    "\n",
    "            # Calculate fractional position of zero crossing\n",
    "            fraction = -y1 / (y2 - y1)\n",
    "\n",
    "            # Calculate precise crossing time\n",
    "            crossing_time = (crossing_idx + fraction) / self.sampling_rate\n",
    "\n",
    "        return cfd_waveform, crossing_time\n",
    "\n",
    "\n",
    "    def get_pulse_timing(self, waveform, threshold=0.1):\n",
    "        \"\"\"Get pulse timing using CFD method with additional validation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        waveform : numpy.ndarray\n",
    "            Input waveform to analyze\n",
    "        threshold : float\n",
    "            Minimum amplitude threshold for valid pulses (relative to max amplitude)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float or None\n",
    "            Timestamp of the pulse in seconds, or None if no valid pulse found\n",
    "        \"\"\"\n",
    "        # Check if pulse amplitude exceeds threshold\n",
    "        max_amplitude = np.max(waveform)\n",
    "        if max_amplitude < threshold:\n",
    "            return None\n",
    "            \n",
    "        # Apply CFD\n",
    "        _, crossing_time = self.apply_cfd(waveform)\n",
    "        \n",
    "        return crossing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "41484993-fa05-4bdd-8fb5-49aa0adfb2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing input for NF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:15<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling took 15.160471200942993 seconds\n",
      "Processing signal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data = processed_data\n",
    "normalizing_flow=model_compile\n",
    "batch_size=50000\n",
    "device='cuda'\n",
    "pixel_threshold = 5\n",
    "processer = SiPMSignalProcessor()\n",
    "\n",
    "pixel_dict = {}\n",
    "\n",
    "all_context = []\n",
    "all_time_pixels = []\n",
    "all_metadata = []\n",
    "num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
    "print(\"Preparing input for NF\")\n",
    "for event_idx, event_data in tqdm(processed_data.items()):\n",
    "    for stave_idx, stave_data in event_data.items():\n",
    "        for layer_idx, layer_data in stave_data.items():\n",
    "            for segment_idx, segment_data in layer_data.items():\n",
    "                trueID_list = []\n",
    "                for particle_id, particle_data in segment_data.items():\n",
    "#                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
    "#                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
    "                    base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
    "                                                dtype=torch.float32)\n",
    "                    base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
    "                                                    dtype=torch.float32)\n",
    "                    base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
    "                                                    dtype=torch.float32)\n",
    "                    if particle_data['trueID'] not in  trueID_list:\n",
    "                        trueID_list.append(particle_data['trueID'])\n",
    "                    for SiPM_idx in range(2):\n",
    "                        z_pos = particle_data['z_pos']\n",
    "                        context = base_context.clone()\n",
    "                        context[0] = z_pos\n",
    "                        num_pixel_tag = num_pixel_list[SiPM_idx]\n",
    "                        all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        if(SiPM_idx == 0):\n",
    "                            all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        else:\n",
    "                            all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
    "                        fields = [\n",
    "                            'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
    "                            'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
    "                            'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
    "                            'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
    "                            'KMU_endpoint_y', 'KMU_endpoint_z'\n",
    "                        ]\n",
    "\n",
    "                        all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
    "                        particle_key = (event_idx,stave_idx,layer_idx,segment_idx)\n",
    "                        if(particle_key in pixel_dict):\n",
    "                            pixel_dict[particle_key][0] +=particle_data[\"num_pixels_high_z\"]\n",
    "                            pixel_dict[particle_key][1] +=particle_data[\"num_pixels_low_z\"]\n",
    "                        else:\n",
    "                            pixel_dict[particle_key] =[particle_data[\"num_pixels_high_z\"],particle_data[\"num_pixels_low_z\"]]\n",
    "all_context = torch.cat(all_context)\n",
    "all_time_pixels = torch.cat(all_time_pixels)\n",
    "\n",
    "print(\"Sampling data...\")\n",
    "sampled_data = []\n",
    "begin = time.time()\n",
    "for i in tqdm(range(0, len(all_context), batch_size)):\n",
    "    batch_end = min(i + batch_size, len(all_context))\n",
    "    batch_context = all_context[i:batch_end].to(device)\n",
    "    batch_time_pixels = all_time_pixels[i:batch_end]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
    "\n",
    "    sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
    "end = time.time()\n",
    "print(f\"sampling took {end - begin} seconds\")\n",
    "print(\"Processing signal...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e571e3b4-6b7f-44a5-a6ee-747df41a3213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DF took 19.41303563117981 seconds\n"
     ]
    }
   ],
   "source": [
    "# VARIABLES FOR SAVING DATA AS DF\n",
    "processer = SiPMSignalProcessor()\n",
    "rows = []\n",
    "\n",
    "seen_keys = set()\n",
    "curr_key = (-1,-1,-1,-1)\n",
    "\n",
    "pixel_counter = np.zeros(2,dtype=int)\n",
    "processor = SiPMSignalProcessor()\n",
    "\n",
    "translated_trueID = 0\n",
    "trueID_dict_running_idx = 0\n",
    "trueID_dict = {}\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "#     sample_idx = 0\n",
    "for (event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, momentum,trueID,truePID,hitID,hitPID,theta,phi,strip_x,strip_y,strip_z,trueID_list_len,hit_x,hit_y,hit_z,KMU_trueID,KMU_truePID,KMU_true_phi,KMU_true_momentum_mag,KMU_endpoint_x,KMU_endpoint_y,KMU_endpoint_z), sample in zip(all_metadata, sampled_data):\n",
    "\n",
    "    #progress bar\n",
    "#         floor_percent = int(np.floor(len(sampled_data) / 100))\n",
    "#         if(sample_idx % floor_percent == 0):\n",
    "#             curr_time = time.time()\n",
    "#             print(f\"Signal Processing is now {int(np.floor(sample_idx / len(sampled_data) * 100))}% complete (time elapsed: {curr_time - begin})\")\n",
    "#             clear_output(wait = True)\n",
    "#         sample_idx += 1\n",
    "\n",
    "    # Work with all samples of one SiPM together\n",
    "    key = (event_idx, stave_idx, layer_idx, segment_idx)\n",
    "    if key in seen_keys:\n",
    "        if key == curr_key:\n",
    "            current_samples[SiPM_idx][pixel_counter[SiPM_idx]] = sample\n",
    "            pixel_counter[SiPM_idx] = pixel_counter[SiPM_idx] + 1\n",
    "        else:\n",
    "            continue\n",
    "            print(f\"ERROR: key: {key} | curr_key: {curr_key}\")\n",
    "    # First key\n",
    "    elif curr_key == (-1,-1,-1,-1):\n",
    "        current_samples = [np.empty(pixel_dict[key][0]),np.empty(pixel_dict[key][1])]\n",
    "        current_samples[SiPM_idx][pixel_counter[SiPM_idx]] = sample\n",
    "        pixel_counter[SiPM_idx] = pixel_counter[SiPM_idx] + 1\n",
    "        seen_keys.add(key)\n",
    "        curr_key = key\n",
    "    # End of curr_key: perform calc\n",
    "    else:\n",
    "        #calculate photon stuff on current_samples\n",
    "\n",
    "        '''IMPLEMENTING PREDICTION INPUT PULSE SEGMENT BY SEGMENT'''\n",
    "        curr_event_idx = curr_key[0]\n",
    "        curr_stave_idx = curr_key[1]\n",
    "        curr_layer_idx = curr_key[2]\n",
    "        curr_segment_idx = curr_key[3]\n",
    "        for curr_SiPM_idx in range(2):\n",
    "            trigger = False\n",
    "            photon_times_not_np = current_samples[curr_SiPM_idx]\n",
    "            photon_times = np.array(photon_times_not_np)\n",
    "            if(len(photon_times) > 0):\n",
    "                time_arr,waveform = processor.generate_waveform(photon_times)\n",
    "                timing = processer.get_pulse_timing(waveform,threshold = pixel_threshold)\n",
    "                if(timing is not None):\n",
    "                    #scale inputs to avoid exploding gradients\n",
    "                    curr_charge = processor.integrate_charge(waveform) / 100\n",
    "                    curr_timing = timing /10\n",
    "                    trigger = True\n",
    "                #skip segments that don't pass the threshold\n",
    "                else:\n",
    "                    continue\n",
    "            #skip segments with no photon hits\n",
    "            else:\n",
    "                continue\n",
    "            if(trueID_list_len > 1):\n",
    "                translated_trueID = -1\n",
    "            else:\n",
    "                if((event_idx,trueID) not in trueID_dict):\n",
    "                    trueID_dict[(event_idx,trueID)] = trueID_dict_running_idx\n",
    "                    trueID_dict_running_idx += 1\n",
    "                translated_trueID = trueID_dict[(event_idx,trueID)]\n",
    "            new_row = {\n",
    "                \"event_idx\"      : curr_event_idx,\n",
    "                \"stave_idx\"      : curr_stave_idx,\n",
    "                \"layer_idx\"      : curr_layer_idx,\n",
    "                \"segment_idx\"    : curr_segment_idx,\n",
    "                \"SiPM_idx\"    : curr_SiPM_idx,\n",
    "                \"trueID\"         : translated_trueID,\n",
    "                \"truePID\"        : trueID,\n",
    "                \"hitID\"          : hitID,\n",
    "                \"P\"              : momentum,\n",
    "                \"Theta\"          : theta,\n",
    "                \"Phi\"            : phi,\n",
    "                \"strip_x\"        : strip_z,\n",
    "                \"strip_y\"        : strip_x,\n",
    "                \"strip_z\"        : strip_y,\n",
    "                \"hit_x\"          : hit_x,\n",
    "                \"hit_y\"          : hit_y,\n",
    "                \"hit_z\"          : hit_z,\n",
    "                \"KMU_endpoint_x\" : KMU_endpoint_x,\n",
    "                \"KMU_endpoint_y\" : KMU_endpoint_y,\n",
    "                \"KMU_endpoint_z\" : KMU_endpoint_z,\n",
    "                \"Charge\"         : curr_charge,\n",
    "                \"Time\"           : curr_timing\n",
    "            }\n",
    "            rows.append(new_row)\n",
    "        ''' END IMPLEMENTATION '''\n",
    "        #reset current samples for new key\n",
    "        seen_keys.add(key)\n",
    "        pixel_counter = pixel_counter = np.zeros(2,dtype=int)\n",
    "        current_samples = [np.empty(pixel_dict[key][0]),np.empty(pixel_dict[key][1])]\n",
    "        current_samples[SiPM_idx][pixel_counter[SiPM_idx]] = sample\n",
    "        pixel_counter[SiPM_idx] = pixel_counter[SiPM_idx] + 1\n",
    "        curr_key = key\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "ret_df = pd.DataFrame(rows)\n",
    "print(f\"Creating DF took {end - begin} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47b6d767-bd9a-40e0-8834-7f712423a4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.64141715e-310, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "       0.00000000e+000, 6.95310027e-310, 4.94065646e-324, 4.94065646e-324,\n",
       "       6.90719599e-310, 6.95310027e-310, 1.48219694e-323, 6.32404027e-322,\n",
       "       4.94065646e-324, 6.90719585e-310, 4.94065646e-324, 0.00000000e+000,\n",
       "       0.00000000e+000, 6.95310027e-310, 0.00000000e+000, 6.90719585e-310,\n",
       "       6.95310027e-310, 6.90709997e-310, 2.12199579e-314, 6.95310027e-310,\n",
       "       2.12199585e-314, 6.95293141e-310, 2.12199579e-314, 6.90709630e-310,\n",
       "       6.95310027e-310, 6.95310027e-310, 7.63918485e-313, 4.94065646e-323,\n",
       "       0.00000000e+000, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "       6.90697860e-310, 1.40107117e+001, 1.71535511e+001, 1.56047277e+001,\n",
       "       1.48282042e+001, 1.63265953e+001, 1.33217020e+001, 1.87257118e+001,\n",
       "       2.18567371e+001, 1.55207739e+001, 1.46341257e+001, 1.59433670e+001,\n",
       "       1.47265110e+001, 1.74372787e+001, 1.68182716e+001, 1.84677734e+001,\n",
       "       1.40344086e+001, 2.80669518e+001, 3.16355972e+001, 2.69963322e+001,\n",
       "       2.29423981e+001, 2.71258106e+001])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_samples[SiPM_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c327791-5549-492c-8cbd-d166ea4ce166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 66])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57809e40-c67e-47bd-b59a-cbf9d510aff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_counter[SiPM_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "596cf9ee-99bf-4000-b8df-4f1704109dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0', '1', '2', '32')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cad37071-46e0-4368-a01d-1683148a8fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SiPM_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf23a10b-c532-47e1-833d-87fa173778d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_dict[('0','1','2','32')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0df2c5a2-9ab6-47c2-8228-b205aa1ace17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_venv",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
