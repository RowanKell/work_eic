{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5e61d4-9181-4950-ba4e-0293caabed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "from io import StringIO\n",
    "from functools import wraps\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pstats\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pympler import asizeof\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from time_res_util import get_compiled_NF_model\n",
    "from momentum_prediction_util import load_defaultdict, SiPMSignalProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453c96b8-3afc-48b2-9da5-bdd69f5c151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_function(func):\n",
    "    \"\"\"\n",
    "    Decorator to profile a specific function using cProfile\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        profiler = cProfile.Profile()\n",
    "        try:\n",
    "            return profiler.runcall(func, *args, **kwargs)\n",
    "        finally:\n",
    "            s = StringIO()\n",
    "            stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n",
    "            stats.print_stats(20)  # Print top 20 time-consuming operations\n",
    "            print(s.getvalue())\n",
    "    return wrapper\n",
    "\n",
    "'''MEMORY PROFILING'''\n",
    "import linecache\n",
    "import os\n",
    "import tracemalloc\n",
    "\n",
    "def display_top(snapshot, key_type='lineno', limit=3):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\n",
    "\n",
    "tracemalloc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4671432-d096-45c6-b89a-1091cc3f240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputProcessedData = \"./data/processed_data/jan_13_new_analyze_100events.json\"\n",
    "model_compile = get_compiled_NF_model()\n",
    "processed_data = load_defaultdict(inputProcessedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40b862ca-5066-4057-82d9-2f334924dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newer_prepare_nn_input(processed_data = processed_data, normalizing_flow=model_compile, batch_size=50000, device='cuda',pixel_threshold = 5):\n",
    "    processer = SiPMSignalProcessor()\n",
    "    \n",
    "    all_context = []\n",
    "    all_time_pixels = []\n",
    "    all_metadata = []\n",
    "    num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
    "    print(\"Preparing input for NF\")\n",
    "    for event_idx, event_data in tqdm(processed_data.items()):\n",
    "        for stave_idx, stave_data in event_data.items():\n",
    "            for layer_idx, layer_data in stave_data.items():\n",
    "                for segment_idx, segment_data in layer_data.items():\n",
    "                    trueID_list = []\n",
    "                    for particle_id, particle_data in segment_data.items():\n",
    "#                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
    "#                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
    "                        base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
    "                                                    dtype=torch.float32)\n",
    "                        base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
    "                                                        dtype=torch.float32)\n",
    "                        base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
    "                                                        dtype=torch.float32)\n",
    "                        if particle_data['trueID'] not in  trueID_list:\n",
    "                            trueID_list.append(particle_data['trueID'])\n",
    "                        for SiPM_idx in range(2):\n",
    "                            z_pos = particle_data['z_pos']\n",
    "                            context = base_context.clone()\n",
    "                            context[0] = z_pos\n",
    "                            num_pixel_tag = num_pixel_list[SiPM_idx]\n",
    "                            all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            if(SiPM_idx == 0):\n",
    "                                all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            else:\n",
    "                                all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
    "                            # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
    "                            fields = [\n",
    "                                'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
    "                                'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
    "                                'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
    "                                'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
    "                                'KMU_endpoint_y', 'KMU_endpoint_z'\n",
    "                            ]\n",
    "\n",
    "                            # Print types of each particle_data field\n",
    "#                             for field in fields:\n",
    "#                                 value = particle_data.get(field, None)\n",
    "#                                 print(f\"{field}: {type(value)}\")\n",
    "\n",
    "#                             # Print the type of len(trueID_list)\n",
    "#                             print(f\"len(trueID_list): {type(len(trueID_list))}\")\n",
    "\n",
    "                            all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
    "\n",
    "    all_context = torch.cat(all_context)\n",
    "    all_time_pixels = torch.cat(all_time_pixels)\n",
    "    \n",
    "    print(\"Sampling data...\")\n",
    "    sampled_data = []\n",
    "    begin = time.time()\n",
    "    for i in tqdm(range(0, len(all_context), batch_size)):\n",
    "        batch_end = min(i + batch_size, len(all_context))\n",
    "        batch_context = all_context[i:batch_end].to(device)\n",
    "        batch_time_pixels = all_time_pixels[i:batch_end]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
    "        \n",
    "        sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
    "    end = time.time()\n",
    "    print(f\"sampling took {end - begin} seconds\")\n",
    "    print(\"Processing signal...\")\n",
    "    \n",
    "    \n",
    "    # VARIABLES FOR SAVING DATA AS DF\n",
    "    processer = SiPMSignalProcessor()\n",
    "    rows = []\n",
    "\n",
    "    seen_keys = set()\n",
    "    curr_key = (-1,-1,-1,-1)\n",
    "\n",
    "    current_samples = [[],[]] \n",
    "    processor = SiPMSignalProcessor()\n",
    "\n",
    "    translated_trueID = 0\n",
    "    trueID_dict_running_idx = 0\n",
    "    trueID_dict = {}\n",
    "\n",
    "    begin = time.time()\n",
    "\n",
    "#     sample_idx = 0\n",
    "    for (event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, momentum,trueID,truePID,hitID,hitPID,theta,phi,strip_x,strip_y,strip_z,trueID_list_len,hit_x,hit_y,hit_z,KMU_trueID,KMU_truePID,KMU_true_phi,KMU_true_momentum_mag,KMU_endpoint_x,KMU_endpoint_y,KMU_endpoint_z), sample in zip(all_metadata, sampled_data):\n",
    "\n",
    "        #progress bar\n",
    "#         floor_percent = int(np.floor(len(sampled_data) / 100))\n",
    "#         if(sample_idx % floor_percent == 0):\n",
    "#             curr_time = time.time()\n",
    "#             print(f\"Signal Processing is now {int(np.floor(sample_idx / len(sampled_data) * 100))}% complete (time elapsed: {curr_time - begin})\")\n",
    "#             clear_output(wait = True)\n",
    "#         sample_idx += 1\n",
    "\n",
    "        # Work with all samples of one SiPM together\n",
    "        key = (event_idx, stave_idx, layer_idx, segment_idx)\n",
    "        if key in seen_keys:\n",
    "            if key == curr_key:\n",
    "                current_samples[SiPM_idx].append(sample)\n",
    "            else:\n",
    "                continue\n",
    "                print(f\"ERROR: key: {key} | curr_key: {curr_key}\")\n",
    "        # First key\n",
    "        elif curr_key == (-1,-1,-1,-1):\n",
    "            current_samples[SiPM_idx].append(sample)\n",
    "            seen_keys.add(key)\n",
    "            curr_key = key\n",
    "        # End of curr_key: perform calc\n",
    "        else:\n",
    "            #calculate photon stuff on current_samples\n",
    "\n",
    "            '''IMPLEMENTING PREDICTION INPUT PULSE SEGMENT BY SEGMENT'''\n",
    "            curr_event_idx = curr_key[0]\n",
    "            curr_stave_idx = curr_key[1]\n",
    "            curr_layer_idx = curr_key[2]\n",
    "            curr_segment_idx = curr_key[3]\n",
    "            for curr_SiPM_idx in range(2):\n",
    "                trigger = False\n",
    "                photon_times = np.array(current_samples[curr_SiPM_idx]) * 10 **(-9)\n",
    "                if(len(photon_times) > 0):\n",
    "                    time_arr,waveform = processor.generate_waveform(photon_times)\n",
    "                    timing = processer.get_pulse_timing(waveform,threshold = pixel_threshold)\n",
    "                    if(timing is not None):\n",
    "                        #scale inputs to avoid exploding gradients\n",
    "                        curr_charge = processor.integrate_charge(waveform) * 1e6\n",
    "                        curr_timing = timing * 1e8\n",
    "                        trigger = True\n",
    "                    #skip segments that don't pass the threshold\n",
    "                    else:\n",
    "                        continue\n",
    "                #skip segments with no photon hits\n",
    "                else:\n",
    "                    continue\n",
    "                if(trueID_list_len > 1):\n",
    "                    translated_trueID = -1\n",
    "                else:\n",
    "                    if((event_idx,trueID) not in trueID_dict):\n",
    "                        trueID_dict[(event_idx,trueID)] = trueID_dict_running_idx\n",
    "                        trueID_dict_running_idx += 1\n",
    "                    translated_trueID = trueID_dict[(event_idx,trueID)]\n",
    "                new_row = {\n",
    "                    \"event_idx\"      : curr_event_idx,\n",
    "                    \"stave_idx\"      : curr_stave_idx,\n",
    "                    \"layer_idx\"      : curr_layer_idx,\n",
    "                    \"segment_idx\"    : curr_segment_idx,\n",
    "                    \"SiPM_idx\"    : curr_SiPM_idx,\n",
    "                    \"trueID\"         : translated_trueID,\n",
    "                    \"truePID\"        : trueID,\n",
    "                    \"hitID\"          : hitID,\n",
    "                    \"P\"              : momentum,\n",
    "                    \"Theta\"          : theta,\n",
    "                    \"Phi\"            : phi,\n",
    "                    \"strip_x\"        : strip_z,\n",
    "                    \"strip_y\"        : strip_x,\n",
    "                    \"strip_z\"        : strip_y,\n",
    "                    \"hit_x\"          : hit_x,\n",
    "                    \"hit_y\"          : hit_y,\n",
    "                    \"hit_z\"          : hit_z,\n",
    "                    \"KMU_endpoint_x\" : KMU_endpoint_x,\n",
    "                    \"KMU_endpoint_y\" : KMU_endpoint_y,\n",
    "                    \"KMU_endpoint_z\" : KMU_endpoint_z,\n",
    "                    \"Charge\"         : curr_charge,\n",
    "                    \"Time\"           : curr_timing\n",
    "                }\n",
    "                rows.append(new_row)\n",
    "            ''' END IMPLEMENTATION '''\n",
    "            #reset current samples for new key\n",
    "            seen_keys.add(key)\n",
    "            current_samples = [[],[]]\n",
    "            current_samples.append(sample)\n",
    "            curr_key = key\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    ret_df = pd.DataFrame(rows)\n",
    "    print(f\"Creating DF took {end - begin} seconds\")\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e0a27c5-a11b-4ef4-98b2-c03c30a2edbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating # photons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:17<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287/287 [03:33<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling took 213.49157738685608 seconds\n",
      "Processing signal...\n",
      "Creating DF took 339.87064504623413 seconds\n",
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 526.664 s\n",
      "File: /tmp/ipykernel_588862/194171182.py\n",
      "Function: newer_prepare_nn_input at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def newer_prepare_nn_input(processed_data = processed_data, normalizing_flow=model_compile, batch_size=50000, device='cuda',pixel_threshold = 5):\n",
      "     2         1    1181332.0    1e+06      0.0      processer = SiPMSignalProcessor()\n",
      "     3                                               \n",
      "     4         1       2541.0   2541.0      0.0      all_context = []\n",
      "     5         1       1788.0   1788.0      0.0      all_time_pixels = []\n",
      "     6         1        790.0    790.0      0.0      all_metadata = []\n",
      "     7         1       1707.0   1707.0      0.0      num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
      "     8         1     155162.0 155162.0      0.0      print(\"Calculating # photons\")\n",
      "     9       100  155872231.0    2e+06      0.0      for event_idx, event_data in tqdm(processed_data.items()):\n",
      "    10       413     679285.0   1644.8      0.0          for stave_idx, stave_data in event_data.items():\n",
      "    11      1717    1985117.0   1156.2      0.0              for layer_idx, layer_data in stave_data.items():\n",
      "    12     11968    9714675.0    811.7      0.0                  for segment_idx, segment_data in layer_data.items():\n",
      "    13     10565    6237158.0    590.4      0.0                      trueID_list = []\n",
      "    14    148752   96960114.0    651.8      0.0                      for particle_id, particle_data in segment_data.items():\n",
      "    15                                           #                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
      "    16                                           #                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
      "    17    276374 1534158529.0   5551.0      0.3                          base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
      "    18    138187   39609427.0    286.6      0.0                                                      dtype=torch.float32)\n",
      "    19    276374 1375826893.0   4978.1      0.3                          base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
      "    20    138187   38633107.0    279.6      0.0                                                          dtype=torch.float32)\n",
      "    21    276374 1360039878.0   4921.0      0.3                          base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
      "    22    138187   38123364.0    275.9      0.0                                                          dtype=torch.float32)\n",
      "    23    138187   62063836.0    449.1      0.0                          if particle_data['trueID'] not in  trueID_list:\n",
      "    24     10565   13040376.0   1234.3      0.0                              trueID_list.append(particle_data['trueID'])\n",
      "    25    414561  301354924.0    726.9      0.1                          for SiPM_idx in range(2):\n",
      "    26    276374  100037801.0    362.0      0.0                              z_pos = particle_data['z_pos']\n",
      "    27    276374 1402272919.0   5073.8      0.3                              context = base_context.clone()\n",
      "    28    276374 1490022567.0   5391.3      0.3                              context[0] = z_pos\n",
      "    29    276374   91398458.0    330.7      0.0                              num_pixel_tag = num_pixel_list[SiPM_idx]\n",
      "    30    276374 2291924985.0   8292.8      0.4                              all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
      "    31    276374  105261859.0    380.9      0.0                              if(SiPM_idx == 0):\n",
      "    32    138187 1592323194.0  11523.0      0.3                                  all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
      "    33                                                                       else:\n",
      "    34    138187 1059633112.0   7668.1      0.2                                  all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
      "    35                                                                       # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
      "    36    276374  271724006.0    983.2      0.1                              fields = [\n",
      "    37                                                                           'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
      "    38                                                                           'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
      "    39                                                                           'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
      "    40                                                                           'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
      "    41                                                                           'KMU_endpoint_y', 'KMU_endpoint_z'\n",
      "    42                                                                       ]\n",
      "    43                                           \n",
      "    44                                                                       # Print types of each particle_data field\n",
      "    45                                           #                             for field in fields:\n",
      "    46                                           #                                 value = particle_data.get(field, None)\n",
      "    47                                           #                                 print(f\"{field}: {type(value)}\")\n",
      "    48                                           \n",
      "    49                                           #                             # Print the type of len(trueID_list)\n",
      "    50                                           #                             print(f\"len(trueID_list): {type(len(trueID_list))}\")\n",
      "    51                                           \n",
      "    52    276374 1231746767.0   4456.8      0.2                              all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
      "    53                                           \n",
      "    54         1  679402047.0    7e+08      0.1      all_context = torch.cat(all_context)\n",
      "    55         1  624229096.0    6e+08      0.1      all_time_pixels = torch.cat(all_time_pixels)\n",
      "    56                                               \n",
      "    57         1     165125.0 165125.0      0.0      print(\"Sampling data...\")\n",
      "    58         1        897.0    897.0      0.0      sampled_data = []\n",
      "    59         1       2786.0   2786.0      0.0      begin = time.time()\n",
      "    60       288  495562241.0    2e+06      0.1      for i in tqdm(range(0, len(all_context), batch_size)):\n",
      "    61       287    8261085.0  28784.3      0.0          batch_end = min(i + batch_size, len(all_context))\n",
      "    62       287  114224244.0 397993.9      0.0          batch_context = all_context[i:batch_end].to(device)\n",
      "    63       287    4159860.0  14494.3      0.0          batch_time_pixels = all_time_pixels[i:batch_end]\n",
      "    64                                                   \n",
      "    65       287   10924986.0  38066.2      0.0          with torch.no_grad():\n",
      "    66       287        2e+11    6e+08     34.7              samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
      "    67                                                   \n",
      "    68       287        3e+10    1e+08      5.7          sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
      "    69         1       2202.0   2202.0      0.0      end = time.time()\n",
      "    70         1      66105.0  66105.0      0.0      print(f\"sampling took {end - begin} seconds\")\n",
      "    71         1      21809.0  21809.0      0.0      print(\"Processing signal...\")\n",
      "    72                                               \n",
      "    73                                               \n",
      "    74                                               # VARIABLES FOR SAVING DATA AS DF\n",
      "    75         1     744513.0 744513.0      0.0      processer = SiPMSignalProcessor()\n",
      "    76         1        893.0    893.0      0.0      rows = []\n",
      "    77                                           \n",
      "    78         1       1822.0   1822.0      0.0      seen_keys = set()\n",
      "    79         1        870.0    870.0      0.0      curr_key = (-1,-1,-1,-1)\n",
      "    80                                           \n",
      "    81         1       2021.0   2021.0      0.0      current_samples = [[],[]] \n",
      "    82         1     116811.0 116811.0      0.0      processor = SiPMSignalProcessor()\n",
      "    83                                           \n",
      "    84         1        498.0    498.0      0.0      translated_trueID = 0\n",
      "    85         1        313.0    313.0      0.0      trueID_dict_running_idx = 0\n",
      "    86         1        614.0    614.0      0.0      trueID_dict = {}\n",
      "    87                                           \n",
      "    88         1       1457.0   1457.0      0.0      begin = time.time()\n",
      "    89                                           \n",
      "    90                                           #     sample_idx = 0\n",
      "    91  14326608 7489734127.0    522.8      1.4      for (event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, momentum,trueID,truePID,hitID,hitPID,theta,phi,strip_x,strip_y,strip_z,trueID_list_len,hit_x,hit_y,hit_z,KMU_trueID,KMU_truePID,KMU_true_phi,KMU_true_momentum_mag,KMU_endpoint_x,KMU_endpoint_y,KMU_endpoint_z), sample in zip(all_metadata, sampled_data):\n",
      "    92                                           \n",
      "    93                                                   #progress bar\n",
      "    94                                           #         floor_percent = int(np.floor(len(sampled_data) / 100))\n",
      "    95                                           #         if(sample_idx % floor_percent == 0):\n",
      "    96                                           #             curr_time = time.time()\n",
      "    97                                           #             print(f\"Signal Processing is now {int(np.floor(sample_idx / len(sampled_data) * 100))}% complete (time elapsed: {curr_time - begin})\")\n",
      "    98                                           #             clear_output(wait = True)\n",
      "    99                                           #         sample_idx += 1\n",
      "   100                                           \n",
      "   101                                                   # Work with all samples of one SiPM together\n",
      "   102  14326607 7448014703.0    519.9      1.4          key = (event_idx, stave_idx, layer_idx, segment_idx)\n",
      "   103  14326607 4936342930.0    344.6      0.9          if key in seen_keys:\n",
      "   104  14316671 4515251364.0    315.4      0.9              if key == curr_key:\n",
      "   105  14316671        1e+10    953.6      2.6                  current_samples[SiPM_idx].append(sample)\n",
      "   106                                                       else:\n",
      "   107                                                           continue\n",
      "   108                                                           print(f\"ERROR: key: {key} | curr_key: {curr_key}\")\n",
      "   109                                                   # First key\n",
      "   110      9936    9211868.0    927.1      0.0          elif curr_key == (-1,-1,-1,-1):\n",
      "   111         1       2052.0   2052.0      0.0              current_samples[SiPM_idx].append(sample)\n",
      "   112         1       1700.0   1700.0      0.0              seen_keys.add(key)\n",
      "   113         1        480.0    480.0      0.0              curr_key = key\n",
      "   114                                                   # End of curr_key: perform calc\n",
      "   115                                                   else:\n",
      "   116                                                       #calculate photon stuff on current_samples\n",
      "   117                                           \n",
      "   118                                                       '''IMPLEMENTING PREDICTION INPUT PULSE SEGMENT BY SEGMENT'''\n",
      "   119      9935    4584622.0    461.5      0.0              curr_event_idx = curr_key[0]\n",
      "   120      9935    3928630.0    395.4      0.0              curr_stave_idx = curr_key[1]\n",
      "   121      9935    3862160.0    388.7      0.0              curr_layer_idx = curr_key[2]\n",
      "   122      9935    3642589.0    366.6      0.0              curr_segment_idx = curr_key[3]\n",
      "   123     29805   39162435.0   1314.0      0.0              for curr_SiPM_idx in range(2):\n",
      "   124     19870    7142561.0    359.5      0.0                  trigger = False\n",
      "   125     19870        1e+11    7e+06     26.4                  photon_times = np.array(current_samples[curr_SiPM_idx]) * 10 **(-9)\n",
      "   126     19870   27176716.0   1367.7      0.0                  if(len(photon_times) > 0):\n",
      "   127     18093        1e+11    6e+06     21.9                      time_arr,waveform = processor.generate_waveform(photon_times)\n",
      "   128     18093 3596953948.0 198803.6      0.7                      timing = processer.get_pulse_timing(waveform,threshold = pixel_threshold)\n",
      "   129     18093   10206381.0    564.1      0.0                      if(timing is not None):\n",
      "   130                                                                   #scale inputs to avoid exploding gradients\n",
      "   131     12205  594520035.0  48711.2      0.1                          curr_charge = processor.integrate_charge(waveform) * 1e6\n",
      "   132     12205   15374764.0   1259.7      0.0                          curr_timing = timing * 1e8\n",
      "   133     12205    6466826.0    529.9      0.0                          trigger = True\n",
      "   134                                                               #skip segments that don't pass the threshold\n",
      "   135                                                               else:\n",
      "   136                                                                   continue\n",
      "   137                                                           #skip segments with no photon hits\n",
      "   138                                                           else:\n",
      "   139                                                               continue\n",
      "   140     12205    5822878.0    477.1      0.0                  if(trueID_list_len > 1):\n",
      "   141                                                               translated_trueID = -1\n",
      "   142                                                           else:\n",
      "   143     12205   19820031.0   1623.9      0.0                      if((event_idx,trueID) not in trueID_dict):\n",
      "   144        98     110268.0   1125.2      0.0                          trueID_dict[(event_idx,trueID)] = trueID_dict_running_idx\n",
      "   145        98      51911.0    529.7      0.0                          trueID_dict_running_idx += 1\n",
      "   146     12205   12014887.0    984.4      0.0                      translated_trueID = trueID_dict[(event_idx,trueID)]\n",
      "   147     12205   46887931.0   3841.7      0.0                  new_row = {\n",
      "   148     12205    4820217.0    394.9      0.0                      \"event_idx\"      : curr_event_idx,\n",
      "   149     12205    5549445.0    454.7      0.0                      \"stave_idx\"      : curr_stave_idx,\n",
      "   150     12205    5046730.0    413.5      0.0                      \"layer_idx\"      : curr_layer_idx,\n",
      "   151     12205    5379625.0    440.8      0.0                      \"segment_idx\"    : curr_segment_idx,\n",
      "   152     12205    4450894.0    364.7      0.0                      \"SiPM_idx\"    : curr_SiPM_idx,\n",
      "   153     12205    4408898.0    361.2      0.0                      \"trueID\"         : translated_trueID,\n",
      "   154     12205    4254314.0    348.6      0.0                      \"truePID\"        : trueID,\n",
      "   155     12205    6259669.0    512.9      0.0                      \"hitID\"          : hitID,\n",
      "   156     12205    5469000.0    448.1      0.0                      \"P\"              : momentum,\n",
      "   157     12205    4674877.0    383.0      0.0                      \"Theta\"          : theta,\n",
      "   158     12205    5567529.0    456.2      0.0                      \"Phi\"            : phi,\n",
      "   159     12205    5180916.0    424.5      0.0                      \"strip_x\"        : strip_z,\n",
      "   160     12205    5316258.0    435.6      0.0                      \"strip_y\"        : strip_x,\n",
      "   161     12205    4505232.0    369.1      0.0                      \"strip_z\"        : strip_y,\n",
      "   162     12205    4749058.0    389.1      0.0                      \"hit_x\"          : hit_x,\n",
      "   163     12205    4902917.0    401.7      0.0                      \"hit_y\"          : hit_y,\n",
      "   164     12205    4627962.0    379.2      0.0                      \"hit_z\"          : hit_z,\n",
      "   165     12205    5379916.0    440.8      0.0                      \"KMU_endpoint_x\" : KMU_endpoint_x,\n",
      "   166     12205    4832058.0    395.9      0.0                      \"KMU_endpoint_y\" : KMU_endpoint_y,\n",
      "   167     12205    4905713.0    401.9      0.0                      \"KMU_endpoint_z\" : KMU_endpoint_z,\n",
      "   168     12205    4556089.0    373.3      0.0                      \"Charge\"         : curr_charge,\n",
      "   169     12205    4742161.0    388.5      0.0                      \"Time\"           : curr_timing\n",
      "   170                                                           }\n",
      "   171     12205   16238545.0   1330.5      0.0                  rows.append(new_row)\n",
      "   172                                                       ''' END IMPLEMENTATION '''\n",
      "   173                                                       #reset current samples for new key\n",
      "   174      9935   17963202.0   1808.1      0.0              seen_keys.add(key)\n",
      "   175      9935  120474812.0  12126.3      0.0              current_samples = [[],[]]\n",
      "   176      9935   17340473.0   1745.4      0.0              current_samples.append(sample)\n",
      "   177      9935    3789717.0    381.5      0.0              curr_key = key\n",
      "   178                                           \n",
      "   179                                           \n",
      "   180         1       2788.0   2788.0      0.0      end = time.time()\n",
      "   181         1   81835144.0    8e+07      0.0      ret_df = pd.DataFrame(rows)\n",
      "   182         1     166443.0 166443.0      0.0      print(f\"Creating DF took {end - begin} seconds\")\n",
      "   183         1        320.0    320.0      0.0      return ret_df\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'profiling/Analyze_def/profile_stats_improved_100events.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m profiler\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewer_prepare_nn_input()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m profiler\u001b[38;5;241m.\u001b[39mprint_stats()\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprofiling/Analyze_def/profile_stats_improved_100events.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     profiler\u001b[38;5;241m.\u001b[39mprint_stats(stream\u001b[38;5;241m=\u001b[39mf)\n",
      "File \u001b[0;32m/hpc/group/vossenlab/rck32/ML_venv/lib64/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'profiling/Analyze_def/profile_stats_improved_100events.txt'"
     ]
    }
   ],
   "source": [
    "from line_profiler import LineProfiler\n",
    "\n",
    "profiler = LineProfiler()\n",
    "profiler.add_function(newer_prepare_nn_input)\n",
    "profiler.run('newer_prepare_nn_input()')\n",
    "profiler.print_stats()\n",
    "with open('profiling/Analyze_dev/profile_stats_improved_100events.txt', 'w') as f:\n",
    "    profiler.print_stats(stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbec99c5-43dc-4b7b-9a90-dbf5169987e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data in new_prepare_nn_input...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:16<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling took 16.723486185073853 seconds\n",
      "Processing signal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "normalizing_flow = model_compile\n",
    "batch_size=50000\n",
    "device='cuda'\n",
    "pixel_threshold = 5\n",
    "\n",
    "all_context = []\n",
    "all_time_pixels = []\n",
    "all_metadata = []\n",
    "num_pixel_list = [\"num_pixels_high_z\",\"num_pixels_low_z\"]\n",
    "print(\"Processing data in new_prepare_nn_input...\")\n",
    "for event_idx, event_data in tqdm(processed_data.items()):\n",
    "    for stave_idx, stave_data in event_data.items():\n",
    "        for layer_idx, layer_data in stave_data.items():\n",
    "            for segment_idx, segment_data in layer_data.items():\n",
    "                trueID_list = []\n",
    "                for particle_id, particle_data in segment_data.items():\n",
    "#                         print(f\"keys of particle data: {particle_data.keys()}\")\n",
    "#                         print(f\"types: {type(particle_data['z_pos'])},{type(particle_data['hittheta'])},{type(particle_data['hitmomentum'])}\")\n",
    "                    base_context = torch.tensor([particle_data['z_pos'], particle_data['hittheta'], particle_data['hitmomentum']], \n",
    "                                                dtype=torch.float32)\n",
    "                    base_time_pixels_low = torch.tensor([particle_data['time'], particle_data['num_pixels_low_z']], \n",
    "                                                    dtype=torch.float32)\n",
    "                    base_time_pixels_high = torch.tensor([particle_data['time'], particle_data['num_pixels_high_z']], \n",
    "                                                    dtype=torch.float32)\n",
    "                    if particle_data['trueID'] not in  trueID_list:\n",
    "                        trueID_list.append(particle_data['trueID'])\n",
    "                    for SiPM_idx in range(2):\n",
    "                        z_pos = particle_data['z_pos']\n",
    "                        context = base_context.clone()\n",
    "                        context[0] = z_pos\n",
    "                        num_pixel_tag = num_pixel_list[SiPM_idx]\n",
    "                        all_context.append(context.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        if(SiPM_idx == 0):\n",
    "                            all_time_pixels.append(base_time_pixels_high.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        else:\n",
    "                            all_time_pixels.append(base_time_pixels_low.repeat(particle_data[num_pixel_tag], 1))\n",
    "                        # Assuming particle_data is a dictionary-like object and trueID_list is defined\n",
    "                        fields = [\n",
    "                            'truemomentum', 'trueID', 'truePID', 'hitID', 'hitPID', \n",
    "                            'truetheta', 'truephi', 'strip_x', 'strip_y', 'strip_z', \n",
    "                            'hit_x', 'hit_y', 'hit_z', 'KMU_trueID', 'KMU_truePID', \n",
    "                            'KMU_true_phi', 'KMU_true_momentum_mag', 'KMU_endpoint_x', \n",
    "                            'KMU_endpoint_y', 'KMU_endpoint_z'\n",
    "                        ]\n",
    "\n",
    "                        # Print types of each particle_data field\n",
    "#                             for field in fields:\n",
    "#                                 value = particle_data.get(field, None)\n",
    "#                                 print(f\"{field}: {type(value)}\")\n",
    "\n",
    "#                             # Print the type of len(trueID_list)\n",
    "#                             print(f\"len(trueID_list): {type(len(trueID_list))}\")\n",
    "\n",
    "                        all_metadata.extend([(event_idx,stave_idx, layer_idx,segment_idx, SiPM_idx, particle_data['truemomentum'],particle_data['trueID'],particle_data['truePID'],particle_data['hitID'],particle_data['hitPID'],particle_data['truetheta'],particle_data['truephi'],particle_data['strip_x'],particle_data['strip_y'],particle_data['strip_z'],len(trueID_list),particle_data['hit_x'],particle_data['hit_y'],particle_data['hit_z'],particle_data['KMU_trueID'],particle_data['KMU_truePID'],particle_data['KMU_true_phi'],particle_data['KMU_true_momentum_mag'],particle_data['KMU_endpoint_x'],particle_data['KMU_endpoint_y'],particle_data['KMU_endpoint_z'])] * particle_data[num_pixel_tag])\n",
    "\n",
    "all_context = torch.cat(all_context)\n",
    "all_time_pixels = torch.cat(all_time_pixels)\n",
    "\n",
    "print(\"Sampling data...\")\n",
    "sampled_data = []\n",
    "begin = time.time()\n",
    "for i in tqdm(range(0, len(all_context), batch_size)):\n",
    "    batch_end = min(i + batch_size, len(all_context))\n",
    "    batch_context = all_context[i:batch_end].to(device)\n",
    "    batch_time_pixels = all_time_pixels[i:batch_end]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context)[0]).squeeze(1)\n",
    "\n",
    "    sampled_data.extend(samples.cpu() + batch_time_pixels[:, 0])\n",
    "end = time.time()\n",
    "print(f\"sampling took {end - begin} seconds\")\n",
    "print(\"Processing signal...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_venv",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
