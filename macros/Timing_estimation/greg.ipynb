{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7dde98-56ff-494a-b9b7-7af39d2ad140",
   "metadata": {},
   "source": [
    "# Processing data for greg clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46367ac-ac69-40eb-ae5b-4704c12d28b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from util import get_layer, theta_func,create_layer_map,phi_func\n",
    "from reco import calculate_num_pixels_z_dependence\n",
    "import matplotlib.pyplot as plot\n",
    "import time\n",
    "from collections import defaultdict\n",
    "# Get device to be used\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import os\n",
    "def checkdir(path):\n",
    "    if not os.path.exists(path): \n",
    "        os.makedirs(path)\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import normflows as nf\n",
    "import datetime\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from momentum_prediction_util import SiPMSignalProcessor\n",
    "\n",
    "#from momentum_prediction_util import process_root_file_for_greg,prepare_nn_input,prepare_prediction_input,Predictor,train,prepare_prediction_input_pulse_for_greg,new_prepare_nn_input_for_greg\n",
    "\n",
    "inputTensorPathName = \"data/greg/input.pt\"\n",
    "outputTensorPathName = \"data/greg/output.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0f72a5-fd36-4084-a223-544560c56e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiPMSignalProcessor:\n",
    "    def __init__(self, \n",
    "                 sampling_rate=40e9,  # 40 GHz sampling rate\n",
    "                 tau_rise=1e-9,       # 1 ns rise time\n",
    "                 tau_fall=50e-9,      # 50 ns fall time\n",
    "                 window=200e-9,       # 200 ns time window\n",
    "                 cfd_delay=5e-9,      # 5 ns delay for CFD\n",
    "                 cfd_fraction=0.3):   # 30% fraction for CFD\n",
    "        \n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.tau_rise = tau_rise\n",
    "        self.tau_fall = tau_fall\n",
    "        self.window = window\n",
    "        self.cfd_delay = cfd_delay\n",
    "        self.cfd_fraction = cfd_fraction\n",
    "        \n",
    "        # Time array for single pulse shape\n",
    "        self.time = np.arange(0, self.window, 1/self.sampling_rate)\n",
    "        \n",
    "        # Generate single pulse shape\n",
    "        self.pulse_shape = self._generate_pulse_shape()\n",
    "    \n",
    "    def _generate_pulse_shape(self):\n",
    "        \"\"\"Generate normalized pulse shape for a single photon\"\"\"\n",
    "        shape = (1 - np.exp(-self.time/self.tau_rise)) * np.exp(-self.time/self.tau_fall)\n",
    "        return shape / np.max(shape)  # Normalize\n",
    "    \n",
    "    def generate_waveform(self, photon_times):\n",
    "        \"\"\"Generate waveform from list of photon arrival times\"\"\"\n",
    "        # Initialize waveform array\n",
    "        waveform = np.zeros_like(self.time)\n",
    "        \n",
    "        # Add pulse for each photon\n",
    "        for t in photon_times:\n",
    "            if 0 <= t < self.window:\n",
    "                idx = int(t * self.sampling_rate)\n",
    "                remaining_samples = len(self.time) - idx\n",
    "                waveform[idx:] += self.pulse_shape[:remaining_samples]\n",
    "        \n",
    "        return self.time, waveform\n",
    "    \n",
    "    def integrate_charge(self, waveform, integration_start=0, integration_time=100e-9):\n",
    "        \"\"\"Integrate charge in specified time window\"\"\"\n",
    "        start_idx = int(integration_start * self.sampling_rate)\n",
    "        end_idx = int((integration_start + integration_time) * self.sampling_rate)\n",
    "        \n",
    "        # Integrate using trapezoidal rule\n",
    "        charge = np.trapezoid(waveform[start_idx:end_idx], dx=1/self.sampling_rate)\n",
    "        return charge\n",
    "    \n",
    "    def cfd_timing(self, waveform):\n",
    "        \"\"\"Implement Constant Fraction Discrimination timing\"\"\"\n",
    "        # Create delayed and attenuated versions\n",
    "        delay_samples = int(self.cfd_delay * self.sampling_rate)\n",
    "        delayed = np.roll(waveform, delay_samples)\n",
    "        attenuated = waveform * self.cfd_fraction\n",
    "        \n",
    "        # CFD waveform\n",
    "        cfd_signal = attenuated - delayed\n",
    "        \n",
    "        # Find zero crossing\n",
    "        zero_crossings = np.where(np.diff(np.signbit(cfd_signal)))[0]\n",
    "        \n",
    "        if len(zero_crossings) > 0:\n",
    "            # Linear interpolation for more precise timing\n",
    "            idx = zero_crossings[0]\n",
    "            t1, t2 = self.time[idx], self.time[idx+1]\n",
    "            v1, v2 = cfd_signal[idx], cfd_signal[idx+1]\n",
    "            \n",
    "            # Time at zero crossing\n",
    "            zero_time = t1 - v1 * (t2 - t1) / (v2 - v1)\n",
    "            return zero_time\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2452d43d-2adf-4d9f-8aa7-80f047774ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/vossenlab/rck32/ML_venv/lib64/python3.9/site-packages/normflows/core.py:213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "Tensor_parent = str(pathlib.Path(outputTensorPathName).parent)\n",
    "\n",
    "# file_name = f\"n_5kevents_0_8_to_10GeV_90theta_origin_file_{file_num}.edm4hep.root\"\n",
    "\n",
    "layer_map, super_layer_map = create_layer_map()\n",
    "\n",
    "x = datetime.datetime.now()\n",
    "today = x.strftime(\"%B_%d\")\n",
    "\n",
    "run_num = 7\n",
    "run_num_str = str(run_num)\n",
    "\n",
    "#NF Stuff\n",
    "\n",
    "K = 8 #num flows\n",
    "\n",
    "latent_size = 1 #dimension of PDF\n",
    "hidden_units = 256 #nodes in hidden layers\n",
    "hidden_layers = 26\n",
    "context_size = 3 #conditional variables for PDF\n",
    "num_context = 3\n",
    "\n",
    "K_str = str(K)\n",
    "batch_size= 2000\n",
    "hidden_units_str = str(hidden_units)\n",
    "hidden_layers_str = str(hidden_layers)\n",
    "batch_size_str = str(batch_size)\n",
    "flows = []\n",
    "for i in range(K):\n",
    "    flows += [nf.flows.AutoregressiveRationalQuadraticSpline(latent_size, hidden_layers, hidden_units, \n",
    "                                                             num_context_channels=context_size)]\n",
    "    flows += [nf.flows.LULinearPermute(latent_size)]\n",
    "\n",
    "# Set base distribution\n",
    "q0 = nf.distributions.DiagGaussian(1, trainable=False)\n",
    "    \n",
    "# Construct flow model\n",
    "model = nf.ConditionalNormalizingFlow(q0, flows)\n",
    "\n",
    "# Move model on GPU if available\n",
    "model = model.to(device)\n",
    "# model_date = \"August_03\"\n",
    "# today = \"August_03\"\n",
    "# model_path = \"models/\" + model_date + \"/\"\n",
    "# checkdir(model_path)\n",
    "\n",
    "model_path = \"/hpc/group/vossenlab/rck32/NF_time_res_models/\"\n",
    "\n",
    "checkdir(Tensor_parent)\n",
    "\n",
    "model.load(model_path + \"run_\" + run_num_str + \"_\" + str(num_context)+ \"context_\" +K_str +  \"flows_\" + hidden_layers_str+\"hl_\" + hidden_units_str+\"hu_\" + batch_size_str+\"bs.pth\")\n",
    "model = model.to(device)\n",
    "model_compile = torch.compile(model,mode = \"reduce-overhead\")\n",
    "model_compile = model_compile.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "476f53b4-ae23-4734-aa7f-63a602e52a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_root_file_for_greg(file_path):\n",
    "    print(\"began processing\")\n",
    "    with uproot.open(file_path) as file:\n",
    "        tree_HcalBarrelHits = file[\"events/HcalBarrelHits\"]\n",
    "        tree_MCParticles = file[\"events/MCParticles\"]\n",
    "        \n",
    "        \n",
    "        momentum_x_MC = tree_MCParticles[\"MCParticles.momentum.x\"].array(library=\"np\")\n",
    "        momentum_y_MC = tree_MCParticles[\"MCParticles.momentum.y\"].array(library=\"np\")\n",
    "        momentum_z_MC = tree_MCParticles[\"MCParticles.momentum.z\"].array(library=\"np\")\n",
    "        \n",
    "        pid_branch = tree_MCParticles[\"MCParticles.PDG\"].array(library=\"np\")\n",
    "        \n",
    "        z_pos = tree_HcalBarrelHits[\"HcalBarrelHits.position.z\"].array(library=\"np\")\n",
    "        x_pos = tree_HcalBarrelHits[\"HcalBarrelHits.position.x\"].array(library=\"np\")\n",
    "        energy = tree_HcalBarrelHits[\"HcalBarrelHits.EDep\"].array(library=\"np\")\n",
    "        momentum_x = tree_HcalBarrelHits[\"HcalBarrelHits.momentum.x\"].array(library=\"np\")\n",
    "        momentum_y = tree_HcalBarrelHits[\"HcalBarrelHits.momentum.y\"].array(library=\"np\")\n",
    "        momentum_z = tree_HcalBarrelHits[\"HcalBarrelHits.momentum.z\"].array(library=\"np\")\n",
    "        hit_time = tree_HcalBarrelHits[\"HcalBarrelHits.time\"].array(library=\"np\")\n",
    "        cellID = tree_HcalBarrelHits[\"HcalBarrelHits.cellID\"].array(library=\"np\")\n",
    "        mc_hit_idx = file[\"events/_HcalBarrelHits_MCParticle/_HcalBarrelHits_MCParticle.index\"].array(library=\"np\")  # Add PDG code for particle identification\n",
    "        print(\"finished loading branches\")\n",
    "        \n",
    "        processed_data = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "        \n",
    "        for event_idx in tqdm(range(len(z_pos))):\n",
    "            if(len(z_pos[event_idx]) == 0):\n",
    "                continue\n",
    "            primary_momentum = (momentum_x_MC[event_idx][0],\n",
    "                            momentum_y_MC[event_idx][0],\n",
    "                            momentum_z_MC[event_idx][0])\n",
    "            primary_momentum_mag = np.linalg.norm(primary_momentum)\n",
    "            if(primary_momentum_mag <= 0):\n",
    "                continue\n",
    "            if(primary_momentum_mag > 100):\n",
    "                continue\n",
    "            energy_per_layer_particle = defaultdict(lambda: defaultdict(float))\n",
    "            first_hit_per_layer_particle = defaultdict(dict)\n",
    "            # First pass: collect first hit data and calculate energy per layer per particle\n",
    "            for hit_idx in range(len(z_pos[event_idx])):\n",
    "                z = z_pos[event_idx][hit_idx]\n",
    "                x = x_pos[event_idx][hit_idx]\n",
    "                e = energy[event_idx][hit_idx]\n",
    "                momentum = (momentum_x[event_idx][hit_idx],\n",
    "                            momentum_y[event_idx][hit_idx],\n",
    "                            momentum_z[event_idx][hit_idx])\n",
    "                momentum_mag = np.linalg.norm(momentum)\n",
    "                theta = theta_func(momentum_x[event_idx][hit_idx], momentum_y[event_idx][hit_idx], momentum_z[event_idx][hit_idx])\n",
    "                phi = phi_func(momentum_x[event_idx][hit_idx], momentum_y[event_idx][hit_idx], momentum_z[event_idx][hit_idx])\n",
    "                layer = get_layer(x)\n",
    "                particle_id = mc_hit_idx[event_idx][hit_idx]\n",
    "                pid = pid_branch[event_idx][particle_id]\n",
    "                curr_cellID = cellID[event_idx][hit_idx]\n",
    "                energy_per_layer_particle[layer][particle_id] += e\n",
    "                \n",
    "                if layer not in first_hit_per_layer_particle or particle_id not in first_hit_per_layer_particle[layer]:\n",
    "                    first_hit_per_layer_particle[layer][particle_id] = {\n",
    "                        \"z_pos\": z,\n",
    "                        \"x_pos\": x,\n",
    "                        \"momentum\": momentum_mag,\n",
    "                        \"primary_momentum\": primary_momentum_mag,\n",
    "                        \"theta\": theta,\n",
    "                        \"time\": hit_time[event_idx][hit_idx],\n",
    "                        \"mc_hit_idx\": particle_id,\n",
    "                        \"pid\": pid,\n",
    "                        \"phi\": phi,\n",
    "                        \"cellID\" : curr_cellID\n",
    "                    }\n",
    "            \n",
    "            \n",
    "            # Second pass: process first hit with total layer energy per particle\n",
    "            for layer, particle_data in first_hit_per_layer_particle.items():\n",
    "                for particle_id, hit_data in particle_data.items():\n",
    "                    layer_particle_energy = energy_per_layer_particle[layer][particle_id]\n",
    "                    num_pixels = calculate_num_pixels_z_dependence(layer_particle_energy, hit_data[\"z_pos\"])\n",
    "#                     print(f\"layer:\\t\\t{layer}\\t|\\tparticle id:\\t{particle_id}\\t|\\tnum_pixels:\\t{num_pixels}\")\n",
    "                    hit_data[\"num_pixels\"] = int(np.floor(num_pixels))\n",
    "                    hit_data[\"layer_energy\"] = layer_particle_energy  # Store total layer energy for this particle\n",
    "                    processed_data[event_idx][layer][particle_id.item()] = hit_data\n",
    "    \n",
    "    print(\"finished processing\")\n",
    "    return processed_data\n",
    "def new_prepare_nn_input_for_greg(processed_data, normalizing_flow, batch_size=1024, device='cuda'):\n",
    "    nn_input = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))\n",
    "    nn_output = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))\n",
    "    \n",
    "    all_context = []\n",
    "    all_time_pixels = []\n",
    "    all_metadata = []\n",
    "    \n",
    "    print(\"Processing data...\")\n",
    "    for event_idx, event_data in tqdm(processed_data.items()):\n",
    "        for layer, layer_data in event_data.items():\n",
    "            for particle_id, particle_data in layer_data.items():\n",
    "                primary_momentum = particle_data[\"primary_momentum\"].item()\n",
    "                base_context = torch.tensor([particle_data['z_pos'], particle_data['theta'], particle_data['momentum']], \n",
    "                                            dtype=torch.float32)\n",
    "                base_time_pixels = torch.tensor([particle_data['time'], particle_data['num_pixels']], \n",
    "                                                dtype=torch.float32)\n",
    "                \n",
    "                for SiPM_idx in range(2):\n",
    "                    z_pos = 1500 - particle_data['z_pos'] if SiPM_idx == 1 else particle_data['z_pos']\n",
    "                    context = base_context.clone()\n",
    "                    context[0] = z_pos\n",
    "                    \n",
    "                    all_context.append(context.repeat(particle_data['num_pixels'], 1))\n",
    "                    all_time_pixels.append(base_time_pixels.repeat(particle_data['num_pixels'], 1))\n",
    "                    all_metadata.extend([(event_idx, layer, SiPM_idx, particle_id, primary_momentum,particle_data['mc_hit_idx'],particle_data['pid'],particle_data['theta'],particle_data['phi'],particle_data['cellID'])] * particle_data['num_pixels'])\n",
    "\n",
    "    all_context = torch.cat(all_context)\n",
    "    all_time_pixels = torch.cat(all_time_pixels)\n",
    "    \n",
    "    print(\"Sampling data...\")\n",
    "    sampled_data = []\n",
    "    for i in tqdm(range(0, len(all_context), batch_size)):\n",
    "        batch_context = all_context[i:i+batch_size].to(device)\n",
    "        batch_time_pixels = all_time_pixels[i:i+batch_size]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            samples = abs(normalizing_flow.sample(num_samples=len(batch_context), context=batch_context[:,:3])[0]).squeeze(1)\n",
    "        \n",
    "        adjusted_times = samples.cpu() + batch_time_pixels[:, 0]\n",
    "        sampled_data.extend(adjusted_times)\n",
    "\n",
    "    print(\"Reorganizing data...\")\n",
    "\n",
    "    for (event, layer, SiPM, particle, momentum,mc_hit_idx,pid,theta,phi,cellID), sample in zip(all_metadata, sampled_data):\n",
    "        nn_input[event][layer][particle][SiPM].append(sample)\n",
    "        nn_output[event][layer][particle][SiPM].append(torch.tensor([momentum,particle,mc_hit_idx,pid,theta,phi,cellID]))\n",
    "    return nn_input, nn_output\n",
    "\n",
    "#TODO Need to get particle idx, theta, phi, p, PID from prepare_prediction_input - don't use new indices, just add as extra values bc we don't need these for my nn training\n",
    "def prepare_prediction_input_pulse_for_greg(nn_input, nn_output):\n",
    "    processor = SiPMSignalProcessor()\n",
    "    \n",
    "    #note - some events do not have dictionaries in nn_input due to being empty\n",
    "    #need to skip over these and condense tensor\n",
    "    out_columns = ['event_idx','layer_idx','trueid','truePID','P','Theta','Phi','cellID','Charge1','Time1','Charge2','Time2']\n",
    "    running_index = 0\n",
    "    row_list = []\n",
    "    \n",
    "    input_dict = defaultdict(lambda: defaultdict(list))\n",
    "    output_dict = {}\n",
    "    curr_event_num = 0\n",
    "    for event_idx in tqdm(list(nn_input)):\n",
    "        event_input = []\n",
    "        set_output = False\n",
    "        layer_keys = nn_input[event_idx].keys()\n",
    "        for layer in range(28):\n",
    "            charge_times = np.empty((2,2))\n",
    "            if(layer in layer_keys): #ignore layers with no hits\n",
    "                particle_keys = nn_input[event_idx][layer].keys()\n",
    "                for particle in particle_keys:\n",
    "                    for SiPM_idx in range(2):\n",
    "                        photon_times = torch.tensor(sorted(nn_input[event_idx][layer][particle][SiPM_idx])) * 10 **(-9)\n",
    "                        #get relative times\n",
    "                        min_time = photon_times[0]\n",
    "                        photon_times = photon_times - min_time\n",
    "\n",
    "                        #calculate time and charge\n",
    "                        time,waveform = processor.generate_waveform(photon_times)\n",
    "                        charge_times[SiPM_idx][0] = processor.integrate_charge(waveform) * 1e6\n",
    "                        charge_times[SiPM_idx][1] = (processor.cfd_timing(waveform) + min_time) * 1e8\n",
    "                    #nn_output[event][layer][particle][SiPM][photon][]\n",
    "                    P = nn_output[event_idx][layer][particle][0][0][0]\n",
    "                    particle_idx = nn_output[event_idx][layer][particle][0][0][2]\n",
    "                    pid = nn_output[event_idx][layer][particle][0][0][3]\n",
    "                    theta = nn_output[event_idx][layer][particle][0][0][4]\n",
    "                    phi = nn_output[event_idx][layer][particle][0][0][5]\n",
    "                    cellID = nn_output[event_idx][layer][particle][0][0][6]\n",
    "                    new_row = [event_idx,layer,particle_idx,pid,P,theta,phi,cellID,charge_times[0,0],charge_times[0,1],charge_times[1,0],charge_times[1,1]]\n",
    "                    new_row_dict = {}\n",
    "                    for i in range(len(new_row)):\n",
    "                        new_row_dict[out_columns[i]] = new_row[i]\n",
    "                    row_list.append(new_row_dict)\n",
    "                    running_index += 1\n",
    "        curr_event_num += 1\n",
    "    return pd.DataFrame(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a024f973-ca44-4980-b074-b818f365a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePathName = \"/hpc/group/vossenlab/rck32/eic/work_eic/root_files/momentum_prediction/October_17/pim_50events_0_8_to_10GeV_90theta_origin_file_0.edm4hep.root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb73f77-74f0-41fe-9c08-3cafe11abfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process_root_file\n",
      "began processing\n",
      "finished loading branches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 182.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing\n",
      "Finished running process_root_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting process_root_file\")\n",
    "processed_data = process_root_file_for_greg(filePathName)\n",
    "print(\"Finished running process_root_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41879fc0-c261-4e60-9b96-149145a1ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prepare_nn_input\n",
      "Processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 50.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:19<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reorganizing data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting prepare_nn_input\")\n",
    "nn_input, nn_output = new_prepare_nn_input_for_greg(processed_data, model_compile,batch_size = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32aeb534-0a70-4618-9f58-29460db9f545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.0588e+00,  0.0000e+00,  0.0000e+00, -2.1100e+02,  8.9862e+01,\n",
       "         2.3037e-01,  1.8347e+19])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_output[0][0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efe5d842-eda0-4776-b5f8-cc428357c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prepare_prediction_input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:52<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting prepare_prediction_input\")\n",
    "out_df = prepare_prediction_input_pulse_for_greg(nn_input,nn_output)\n",
    "print(\"finished job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8f1a268-cfd6-4bbc-a380-f97540e7da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv(\"data/greg/pim_50events_0_8_to_10GeV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0890752-f74f-4f14-adbe-878e20ae2a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_idx</th>\n",
       "      <th>layer_idx</th>\n",
       "      <th>trueid</th>\n",
       "      <th>truePID</th>\n",
       "      <th>P</th>\n",
       "      <th>Theta</th>\n",
       "      <th>Phi</th>\n",
       "      <th>cellID</th>\n",
       "      <th>Charge1</th>\n",
       "      <th>Time1</th>\n",
       "      <th>Charge2</th>\n",
       "      <th>Time2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tensor(0.)</td>\n",
       "      <td>tensor(-211.)</td>\n",
       "      <td>tensor(5.0588)</td>\n",
       "      <td>tensor(89.8623)</td>\n",
       "      <td>tensor(0.2304)</td>\n",
       "      <td>tensor(1.8347e+19)</td>\n",
       "      <td>3.316378</td>\n",
       "      <td>1.177483</td>\n",
       "      <td>3.260657</td>\n",
       "      <td>0.795027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tensor(73.)</td>\n",
       "      <td>tensor(2112.)</td>\n",
       "      <td>tensor(5.0588)</td>\n",
       "      <td>tensor(78.0706)</td>\n",
       "      <td>tensor(52.6191)</td>\n",
       "      <td>tensor(8.3035e+17)</td>\n",
       "      <td>0.139199</td>\n",
       "      <td>1.814522</td>\n",
       "      <td>0.139545</td>\n",
       "      <td>2.120481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tensor(75.)</td>\n",
       "      <td>tensor(2112.)</td>\n",
       "      <td>tensor(5.0588)</td>\n",
       "      <td>tensor(38.7456)</td>\n",
       "      <td>tensor(27.4640)</td>\n",
       "      <td>tensor(1.3314e+18)</td>\n",
       "      <td>0.137349</td>\n",
       "      <td>4.900336</td>\n",
       "      <td>0.139508</td>\n",
       "      <td>4.781552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tensor(77.)</td>\n",
       "      <td>tensor(2212.)</td>\n",
       "      <td>tensor(5.0588)</td>\n",
       "      <td>tensor(47.6523)</td>\n",
       "      <td>tensor(-74.7821)</td>\n",
       "      <td>tensor(1.3342e+18)</td>\n",
       "      <td>0.370849</td>\n",
       "      <td>4.887446</td>\n",
       "      <td>0.372010</td>\n",
       "      <td>4.878765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tensor(76.)</td>\n",
       "      <td>tensor(2212.)</td>\n",
       "      <td>tensor(5.0588)</td>\n",
       "      <td>tensor(116.7846)</td>\n",
       "      <td>tensor(76.4788)</td>\n",
       "      <td>tensor(1.3291e+18)</td>\n",
       "      <td>0.785211</td>\n",
       "      <td>4.828413</td>\n",
       "      <td>0.778875</td>\n",
       "      <td>4.591859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15904</th>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>tensor(560.)</td>\n",
       "      <td>tensor(2212.)</td>\n",
       "      <td>tensor(3.6879)</td>\n",
       "      <td>tensor(103.4935)</td>\n",
       "      <td>tensor(-11.2493)</td>\n",
       "      <td>tensor(1.8147e+19)</td>\n",
       "      <td>6.957597</td>\n",
       "      <td>1.505618</td>\n",
       "      <td>6.836935</td>\n",
       "      <td>1.493273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15905</th>\n",
       "      <td>49</td>\n",
       "      <td>24</td>\n",
       "      <td>tensor(560.)</td>\n",
       "      <td>tensor(2212.)</td>\n",
       "      <td>tensor(3.6879)</td>\n",
       "      <td>tensor(106.6034)</td>\n",
       "      <td>tensor(-13.5247)</td>\n",
       "      <td>tensor(1.8059e+19)</td>\n",
       "      <td>9.499663</td>\n",
       "      <td>1.560827</td>\n",
       "      <td>9.292064</td>\n",
       "      <td>1.447770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15906</th>\n",
       "      <td>49</td>\n",
       "      <td>25</td>\n",
       "      <td>tensor(560.)</td>\n",
       "      <td>tensor(2212.)</td>\n",
       "      <td>tensor(3.6879)</td>\n",
       "      <td>tensor(106.2941)</td>\n",
       "      <td>tensor(-13.5575)</td>\n",
       "      <td>tensor(1.8043e+19)</td>\n",
       "      <td>9.342930</td>\n",
       "      <td>1.585275</td>\n",
       "      <td>9.161994</td>\n",
       "      <td>1.467192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15907</th>\n",
       "      <td>49</td>\n",
       "      <td>25</td>\n",
       "      <td>tensor(571.)</td>\n",
       "      <td>tensor(11.)</td>\n",
       "      <td>tensor(3.6879)</td>\n",
       "      <td>tensor(137.0754)</td>\n",
       "      <td>tensor(-21.2075)</td>\n",
       "      <td>tensor(1.8029e+19)</td>\n",
       "      <td>0.597342</td>\n",
       "      <td>1.726399</td>\n",
       "      <td>0.592822</td>\n",
       "      <td>1.425228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15908</th>\n",
       "      <td>49</td>\n",
       "      <td>25</td>\n",
       "      <td>tensor(570.)</td>\n",
       "      <td>tensor(11.)</td>\n",
       "      <td>tensor(3.6879)</td>\n",
       "      <td>tensor(105.1550)</td>\n",
       "      <td>tensor(-48.0710)</td>\n",
       "      <td>tensor(1.8041e+19)</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>1.553702</td>\n",
       "      <td>0.542650</td>\n",
       "      <td>1.378948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15909 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       event_idx  layer_idx        trueid        truePID               P  \\\n",
       "0              0          0    tensor(0.)  tensor(-211.)  tensor(5.0588)   \n",
       "1              0          0   tensor(73.)  tensor(2112.)  tensor(5.0588)   \n",
       "2              0          0   tensor(75.)  tensor(2112.)  tensor(5.0588)   \n",
       "3              0          0   tensor(77.)  tensor(2212.)  tensor(5.0588)   \n",
       "4              0          0   tensor(76.)  tensor(2212.)  tensor(5.0588)   \n",
       "...          ...        ...           ...            ...             ...   \n",
       "15904         49         23  tensor(560.)  tensor(2212.)  tensor(3.6879)   \n",
       "15905         49         24  tensor(560.)  tensor(2212.)  tensor(3.6879)   \n",
       "15906         49         25  tensor(560.)  tensor(2212.)  tensor(3.6879)   \n",
       "15907         49         25  tensor(571.)    tensor(11.)  tensor(3.6879)   \n",
       "15908         49         25  tensor(570.)    tensor(11.)  tensor(3.6879)   \n",
       "\n",
       "                  Theta               Phi              cellID   Charge1  \\\n",
       "0       tensor(89.8623)    tensor(0.2304)  tensor(1.8347e+19)  3.316378   \n",
       "1       tensor(78.0706)   tensor(52.6191)  tensor(8.3035e+17)  0.139199   \n",
       "2       tensor(38.7456)   tensor(27.4640)  tensor(1.3314e+18)  0.137349   \n",
       "3       tensor(47.6523)  tensor(-74.7821)  tensor(1.3342e+18)  0.370849   \n",
       "4      tensor(116.7846)   tensor(76.4788)  tensor(1.3291e+18)  0.785211   \n",
       "...                 ...               ...                 ...       ...   \n",
       "15904  tensor(103.4935)  tensor(-11.2493)  tensor(1.8147e+19)  6.957597   \n",
       "15905  tensor(106.6034)  tensor(-13.5247)  tensor(1.8059e+19)  9.499663   \n",
       "15906  tensor(106.2941)  tensor(-13.5575)  tensor(1.8043e+19)  9.342930   \n",
       "15907  tensor(137.0754)  tensor(-21.2075)  tensor(1.8029e+19)  0.597342   \n",
       "15908  tensor(105.1550)  tensor(-48.0710)  tensor(1.8041e+19)  0.552816   \n",
       "\n",
       "          Time1   Charge2     Time2  \n",
       "0      1.177483  3.260657  0.795027  \n",
       "1      1.814522  0.139545  2.120481  \n",
       "2      4.900336  0.139508  4.781552  \n",
       "3      4.887446  0.372010  4.878765  \n",
       "4      4.828413  0.778875  4.591859  \n",
       "...         ...       ...       ...  \n",
       "15904  1.505618  6.836935  1.493273  \n",
       "15905  1.560827  9.292064  1.447770  \n",
       "15906  1.585275  9.161994  1.467192  \n",
       "15907  1.726399  0.592822  1.425228  \n",
       "15908  1.553702  0.542650  1.378948  \n",
       "\n",
       "[15909 rows x 12 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4fbdeac7-7a4c-4542-b5f6-90414be48586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.36357142857143"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_df) / 50 / 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a43cf0-1703-4606-9f9b-947681522e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_venv",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
