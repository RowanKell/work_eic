{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb51bacc-90bc-4735-a49e-73563694b42c",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a445e11-8019-4d45-876b-2e162b369ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import uproot as up\n",
    "import numba as nb\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import curve_fit\n",
    "import sympy\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "import time\n",
    "import util\n",
    "#My imports\n",
    "from util import PVect,get_layer,create_layer_map,theta_func,phi_func,findBin,bin_percent_theta_phi, train, test, create_data, create_data_depth,p_func, calculate_num_pixels,Classifier,plot_roc_curve\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a066da77-8534-442d-adc4-b1e3c115e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "particle = \"mu\"\n",
    "energy = \"5\"\n",
    "color_dict = {\n",
    "    \"pi\" : \"red\",\n",
    "    \"mu\" : \"blue\"\n",
    "}\n",
    "part_dict = {\n",
    "    -211 : 1,\n",
    "    13 : 0\n",
    "}\n",
    "layer_map, super_layer_map = create_layer_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "402621bb-bdc3-405b-a4bb-5ed749bad5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @nb.njit\n",
    "def inverse(x, a, b, c):\n",
    "    return a / (x + b) + c\n",
    "\n",
    "# @nb.njit\n",
    "def calculate_num_pixels_z_dependence(energy_dep, z_hit):\n",
    "    efficiency = inverse(770 - z_hit, 494.98, 9.9733, -0.16796)\n",
    "    return 10 * energy_dep * (1000 * 1000) * efficiency / 100\n",
    "def calculate_efficiency(z_hit):\n",
    "    return inverse(770 - z_hit, 494.98, 9.9733, -0.16796)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "39aa23b1-4737-47f3-a455-25e4874e98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 28\n",
    "\n",
    "def get_label(PDG):\n",
    "    return (PDG + 211) // 224\n",
    "\n",
    "def create_unique_mapping(arr):\n",
    "    # Get unique values and their inverse mapping\n",
    "    unique_values, inverse_indices = np.unique(arr, return_inverse=True)\n",
    "    \n",
    "    # Create a dictionary mapping unique values to their indices\n",
    "    value_to_index = {val: idx for idx, val in enumerate(unique_values)}\n",
    "    \n",
    "    # Create an array of indices\n",
    "    index_array = inverse_indices\n",
    "    \n",
    "    return len(unique_values), value_to_index\n",
    "\n",
    "def process_data(uproot_path, file_num=0, particle=\"pion\"):\n",
    "    data = []\n",
    "    events = up.open(uproot_path)\n",
    "    \n",
    "    x_pos_branch = events[\"HcalBarrelHits/HcalBarrelHits.position.x\"].array(library='np')\n",
    "    z_pos_branch = events[\"HcalBarrelHits/HcalBarrelHits.position.z\"].array(library='np')\n",
    "    EDep_branch = events[\"HcalBarrelHits.EDep\"].array(library='np')\n",
    "    PDG_branch = events[\"MCParticles.PDG\"].array(library='np')\n",
    "    x_momentum_branch = events[\"HcalBarrelHits/HcalBarrelHits.momentum.x\"].array(library='np')\n",
    "    y_momentum_branch = events[\"HcalBarrelHits/HcalBarrelHits.momentum.y\"].array(library='np')\n",
    "    z_momentum_branch = events[\"HcalBarrelHits/HcalBarrelHits.momentum.z\"].array(library='np')\n",
    "    Hits_MC_idx_branch = events[\"_HcalBarrelHits_MCParticle.index\"].array(library='np')\n",
    "    time_branch = events[\"HcalBarrelHits.time\"].array(library='np')   \n",
    "    num_events = len(x_pos_branch)\n",
    "    for event_idx in range(num_events):\n",
    "        Hits_MC_idx_event = Hits_MC_idx_branch[event_idx]\n",
    "        n_unique_parts, idx_dict = create_unique_mapping(Hits_MC_idx_event)\n",
    "        \n",
    "        p_layer_list = np.ones((n_unique_parts,num_layers)) * -1\n",
    "        z_hit_layer_list = np.ones((n_unique_parts,num_layers)) * -1\n",
    "        theta_layer_list = np.ones((n_unique_parts,num_layers)) * -1\n",
    "        hit_time_layer_list = np.ones((n_unique_parts,num_layers)) * -1\n",
    "        edep_event = np.ones((n_unique_parts,num_layers)) * -1\n",
    "        \n",
    "        x_pos_event = x_pos_branch[event_idx]\n",
    "        px_event = x_momentum_branch[event_idx]\n",
    "        py_event = y_momentum_branch[event_idx]\n",
    "        pz_event = z_momentum_branch[event_idx]\n",
    "        z_event = z_pos_branch[event_idx]\n",
    "        time_event = time_branch[event_idx]\n",
    "        EDep_event = EDep_branch[event_idx]\n",
    "        for hit_idx in range(len(x_pos_event)):\n",
    "            idx = Hits_MC_idx_branch[event_idx][hit_idx]\n",
    "            part_idx = idx_dict[idx]\n",
    "            layer_idx = get_layer(x_pos_event[hit_idx], super_layer_map)\n",
    "            if layer_idx == -1: #error handling for get_layer\n",
    "                continue\n",
    "            elif p_layer_list[part_idx,layer_idx] == -1:\n",
    "                p_layer_list[part_idx,layer_idx] = np.sqrt(px_event[hit_idx]**2 + py_event[hit_idx]**2 + pz_event[hit_idx]**2)\n",
    "                z_hit_layer_list[part_idx,layer_idx] = z_event[hit_idx]\n",
    "                theta_layer_list[part_idx,layer_idx] = np.arctan2(np.sqrt(px_event[hit_idx]**2 + py_event[hit_idx]**2), pz_event[hit_idx])\n",
    "                hit_time_layer_list[part_idx,layer_idx] = time_event[hit_idx]\n",
    "                edep_event[part_idx,layer_idx] = EDep_event[hit_idx]\n",
    "            else:\n",
    "                edep_event[part_idx,layer_idx] += EDep_event[hit_idx]\n",
    "        data.append(np.stack([z_hit_layer_list,hit_time_layer_list,theta_layer_list,p_layer_list,(np.floor(calculate_num_pixels_z_dependence(edep_event,z_hit_layer_list)).astype(int))],axis = -1))\n",
    "\n",
    "\n",
    "    \n",
    "    return data #returns list: each entry is a diff event array; each event array has shape: (#unique particles, #layers, #features)\n",
    "                #features: z hit, hit time, theta, p, energy dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e3d45f5e-4229-45e7-a889-a21038b074b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_path = f\"/cwork/rck32/eic/work_eic/root_files/June_18/variation_sector_scint_uniform/mu/variation_10kevents_file_11.edm4hep.root:events\"\n",
    "data = process_data(up_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b249ab44-4279-4e5e-a976-f3d7c9bc7014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (72494710, 4)\n",
      "Metadata shape: (72494710, 3)\n",
      "Batch features shape: torch.Size([32, 4])\n",
      "Batch metadata shape: torch.Size([32, 3])\n",
      "Datapoint 0 in batch: Event 9585, Particle 0, Layer 11\n",
      "Datapoint 1 in batch: Event 1173, Particle 0, Layer 10\n",
      "Datapoint 2 in batch: Event 6061, Particle 5, Layer 11\n",
      "Datapoint 3 in batch: Event 8351, Particle 2, Layer 3\n",
      "Datapoint 4 in batch: Event 8820, Particle 2, Layer 5\n",
      "Datapoint 5 in batch: Event 7034, Particle 0, Layer 4\n",
      "Datapoint 6 in batch: Event 5294, Particle 0, Layer 15\n",
      "Datapoint 7 in batch: Event 5774, Particle 0, Layer 12\n",
      "Datapoint 8 in batch: Event 7034, Particle 1, Layer 2\n",
      "Datapoint 9 in batch: Event 9057, Particle 31, Layer 22\n",
      "Datapoint 10 in batch: Event 8364, Particle 0, Layer 4\n",
      "Datapoint 11 in batch: Event 6128, Particle 0, Layer 25\n",
      "Datapoint 12 in batch: Event 8372, Particle 0, Layer 11\n",
      "Datapoint 13 in batch: Event 6519, Particle 0, Layer 19\n",
      "Datapoint 14 in batch: Event 1277, Particle 0, Layer 5\n",
      "Datapoint 15 in batch: Event 8847, Particle 6, Layer 14\n",
      "Datapoint 16 in batch: Event 3744, Particle 12, Layer 21\n",
      "Datapoint 17 in batch: Event 3529, Particle 0, Layer 9\n",
      "Datapoint 18 in batch: Event 2127, Particle 0, Layer 15\n",
      "Datapoint 19 in batch: Event 3655, Particle 0, Layer 2\n",
      "Datapoint 20 in batch: Event 760, Particle 0, Layer 4\n",
      "Datapoint 21 in batch: Event 3298, Particle 0, Layer 8\n",
      "Datapoint 22 in batch: Event 1334, Particle 0, Layer 9\n",
      "Datapoint 23 in batch: Event 1475, Particle 0, Layer 20\n",
      "Datapoint 24 in batch: Event 4338, Particle 0, Layer 24\n",
      "Datapoint 25 in batch: Event 608, Particle 5, Layer 22\n",
      "Datapoint 26 in batch: Event 9283, Particle 0, Layer 3\n",
      "Datapoint 27 in batch: Event 8468, Particle 0, Layer 7\n",
      "Datapoint 28 in batch: Event 9695, Particle 0, Layer 13\n",
      "Datapoint 29 in batch: Event 3175, Particle 0, Layer 20\n",
      "Datapoint 30 in batch: Event 4559, Particle 6, Layer 8\n",
      "Datapoint 31 in batch: Event 3854, Particle 0, Layer 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def prepare_data_for_nn(processed_data):\n",
    "    all_features = []\n",
    "    all_metadata = []\n",
    "    \n",
    "    for event_idx, event_data in enumerate(processed_data):\n",
    "        for particle_idx in range(event_data.shape[0]):\n",
    "            for layer_idx in range(event_data.shape[1]):\n",
    "                features = event_data[particle_idx, layer_idx, :4]  # Get first 4 features\n",
    "                repeat_count = int(event_data[particle_idx, layer_idx, 4])  # Get 5th feature as repeat count\n",
    "                \n",
    "                if not np.any(features == -1) and repeat_count > 0:  # Check if all features are -1 and repeat_count is valid\n",
    "                    # Repeat the features and metadata by repeat_count\n",
    "                    all_features.extend([features] * repeat_count)\n",
    "                    all_metadata.extend([(event_idx, particle_idx, layer_idx)] * repeat_count)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    features_array = np.array(all_features)\n",
    "    metadata_array = np.array(all_metadata)\n",
    "    \n",
    "    return features_array, metadata_array\n",
    "\n",
    "def create_dataloader(features, metadata, batch_size=32):\n",
    "    # Convert to PyTorch tensors\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "    metadata_tensor = torch.tensor(metadata, dtype=torch.long)\n",
    "    \n",
    "    # Create TensorDataset\n",
    "    dataset = TensorDataset(features_tensor, metadata_tensor)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# Assuming processed_data is your list of 3D numpy arrays from the previous step\n",
    "features, metadata = prepare_data_for_nn(data)\n",
    "\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Metadata shape:\", metadata.shape)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "dataloader = create_dataloader(features, metadata, batch_size)\n",
    "\n",
    "# Example of using the DataLoader\n",
    "for batch_features, batch_metadata in dataloader:\n",
    "    print(\"Batch features shape:\", batch_features.shape)\n",
    "    print(\"Batch metadata shape:\", batch_metadata.shape)\n",
    "    \n",
    "    # Here you would typically pass batch_features to your neural network\n",
    "    \n",
    "    # You can access the metadata for each datapoint in the batch like this:\n",
    "    for i in range(batch_metadata.shape[0]):\n",
    "        event_idx, particle_idx, layer_idx = batch_metadata[i]\n",
    "        print(f\"Datapoint {i} in batch: Event {event_idx}, Particle {particle_idx}, Layer {layer_idx}\")\n",
    "    \n",
    "    break  # Just to demonstrate one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8eb9f2b6-8126-41ac-a5f2-d40deb231508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f6f70a3cbe0>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38192896-3e42-493f-89ea-784f4f5e35da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_venv",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
